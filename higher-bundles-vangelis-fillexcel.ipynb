{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# ASSUMPTIONS\n# KEEP HIGHEST BUNDLE VALUE FOR CUSTOMERS THAT PURCHASED MORE THAN ONE BUNDLE\n# CONSIDER CUSTOMERS THAT ARE MORE 3 MONTHS IN OUR BASE (TENURE >3) & 1 MONTH ACTIVE"}, {"cell_type": "markdown", "metadata": {}, "source": "### Table of Contents\n* [Import libraries](#import_libraries)\n* [Functions](#functions)\n* [DF1](#df1)\n    1. [Higher_Bundle_Migrators](#Higher_Bundle_Migrators)\n    2. [gdpr](#gdpr)\n    3. [status](#status)\n    4. [usage](#usage)\n    5. [Add/Convert_Features](#Add/Convert_Features)\n    6. [demographics](#demographics)\n    7. [post_code](#post_code)\n    8. [students](#students)\n    9. [buckets](#buckets)\n    10. [balance](#balance)\n    11. [drop_calls](#drop_calls)\n    12. [tickets](#tickets)\n    13. [channel](#channel)\n    14. [NumberOfBundles](#NumberOfBundles)\n* [DF2](#df2)\n    1. [Higher_Bundle_Migrators2](#Higher_Bundle_Migrators2)\n    2. [gdpr2](#gdpr2)\n    3. [status2](#status2)\n    4. [usage2](#usage2)\n    5. [Add/Convert_Features2](#Add/Convert_Features2)\n    6. [demographics2](#demographics2)\n    7. [post_code2](#post_code2)\n    8. [students2](#students2)\n    9. [buckets2](#buckets2)\n    10. [balance2](#balance2)\n    11. [drop_calls2](#drop_calls2)\n    12. [tickets2](#tickets2)\n    13. [channel2](#channel2)\n    14. [NumberOfBundles2](#NumberOfBundles2)\n* [Modelling](#modelling)\n    1. [Analysis](#Analysis)\n    2. [Preprocessing](#Preprocessing)\n    3. [Training](#Training)\n    4. [Evaluation](#Evaluation)\n    5. [Grid_Search](#Grid_Search)\n    6. [Tuning](#Tuning)\n* [Scoring_dataset](#Scoring_dataset)\n    1. [gdpr3](#gdpr3)\n    2. [status3](#status3)\n    3. [usage3](#usage3)\n    4. [Add/Convert_Features3](#Add/Convert_Features3)\n    5. [demographics3](#demographics3)\n    6. [post_code3](#post_code3)\n    7. [students3](#students3)\n    8. [buckets3](#buckets3)\n    9. [balance3](#balance3)\n    10. [drop_calls3](#drop_calls3)\n    11. [tickets3](#tickets3)\n    12. [channel3](#channel3)\n    13. [NumberOfBundles3](#NumberOfBundles3)\n* [Score](#score)\n* [Save_model](#save_model)"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "#Set parameters\n#input\ncustomerprofilecar_rawprepared_bucket = \"vfgr-dh-customerprofilecar-rawprepared\"\npermsandprefs_rawprepared_bucket = \"vfgr-dh-permsandprefs-rawprepared\"\nmediatedcdrs_bucket = \"vfgr-dh-mediatedcdrs-rawprepared\"\ncellinventory_bucket = \"vfgr-dh-cellinventory-rawprepared\"\nmodel_outputs_bucket = \"vfgr-dh-ca-modeloutputs\"\ndhdwh_bucket = \"vfgr-dh-dwh-rawprepared\"\nmedallia_bucket = 'vfgr-dh-medallia-rawprepared'\n\n# Live project\n#files_bucket = \"vf-gr-ca-live-proda\"\n#output_bucket_new = \"vfgr-dh-ca-modeloutputs\"\n\n# NonLive project\nfiles_bucket = \"vf-gr-ca-nonlive-devde\"\noutput_bucket_new = \"vf-gr-ca-nonlive-devds/modeloutputs\"\n\nversion = \"2.0\""}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "gdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/'\nstatus = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/'\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/'\n\ndemographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/'\nline = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/day={}/'\n\nbuckets = 'gs://'+ model_outputs_bucket + '/prepay_buckets/result/parquet/1.0/year={}/month={}/'\n\nstatus_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\nticket_service = 'gs://'+ dhdwh_bucket +'/mobile_sr_tt/1.0/parquet/year={}/month={}/'\n\ncdrs = 'gs://'+ mediatedcdrs_bucket +'/eds_network_cdr/2.0/parquet/year={}/month={}/'\nevents = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'"}, {"cell_type": "markdown", "metadata": {}, "source": "### Import_libaries <a class=\"anchor\" id=\"import_libraries\"></a>"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport datetime\nfrom dateutil.relativedelta import relativedelta\nimport calendar\nfrom functools import reduce\nimport datetime\nfrom pyspark.sql.functions import expr\nfrom past.builtins import xrange\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Window\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import udf, split, col, count, sum, coalesce, lit, avg\nfrom pyspark.sql.functions import regexp_replace, col\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.functions import format_number, format_string, dayofmonth, hour, month, year, date_format, countDistinct\nfrom pyspark.sql.functions import rank, col, row_number, mean, stddev\nfrom pyspark.sql.functions import isnan, when, count, col, round\nfrom pyspark.sql.functions import desc\nfrom pyspark.sql.functions import substring\nfrom pyspark.sql.types import StructField, StringType, IntegerType, StructType\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import mean, min, max\nimport subprocess\nimport math\nfrom time import time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\npd.set_option('display.expand_frame_repr', True)\n\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.conf import SparkConf\n\nimport findspark\nfindspark.init()\nimport pyspark\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext \nsc = SparkContext.getOrCreate()\nsql = SQLContext(sc)\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import classification_report\nimport shap"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\" "}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://gr-cluster-irida-cbu-m.c.vf-gr-ca-nonlive.internal:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.8</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f3726236cf8>"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "# Get spark\nconfig = SparkConf().setAll([\n        ('spark.driver.port', '59600'),\n        ('spark.blockManager.port', '59601'),\n        ('spark.broadcast.port', '59602'),\n        ('spark.replClassServer', '59603'),\n        ('spark.ui.port', '59604'),\n        ('spark.executor.port', '59605'),\n        ('spark.fileserver.port', '59606'),\n        ('spark.yarn.executor.memoryOverhead', 4096),\n        ('spark.yarn.driver.memoryOverhead', 6144)#,\n    #('spark.sql.session.timeZone','EET')\n    ])\n\nspark = SparkSession.builder.config(conf=config).getOrCreate()\nspark"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Go\n"}], "source": "print('Go')"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1674230119.1559803\n"}], "source": "import time\nstart_time = time.time()\nprint(start_time)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "import datetime\ncurrentDay = datetime.date.today().day\ncurrentMonth = 12\ncurrentYear = 2022"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "20 12 2022\n"}], "source": "print(currentDay, currentMonth, currentYear)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "2022-12-01 00:00:00\n"}], "source": "project_date = datetime.datetime(currentYear, currentMonth, 1)\nprint(project_date)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[11] [2022]\n[11, 10] [2022, 2022]\n[11, 10, 9] [2022, 2022, 2022]\n[11, 10, 9, 8] [2022, 2022, 2022, 2022]\n[11, 10, 9, 8, 7] [2022, 2022, 2022, 2022, 2022]\n[11, 10, 9, 8, 7, 6] [2022, 2022, 2022, 2022, 2022, 2022]\n[11, 10, 9, 8, 7, 6, 5] [2022, 2022, 2022, 2022, 2022, 2022, 2022]\n[11, 10, 9, 8, 7, 6, 5, 4] [2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022]\n"}], "source": "from dateutil.relativedelta import relativedelta\nyear_val = []\nmonth_val = []\nfor x in range (1,9):\n        month = (project_date+relativedelta(months=-x)).date().month\n        year = (project_date+relativedelta(months=-x)).date().year\n        month_val.append(month)\n        year_val.append(year)\n        print(month_val, year_val)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Functions <a class=\"anchor\" id=\"functions\"></a>"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "# Read data from a specific month,year\n\ndef read_in_data(in_file, year = None, month = None):\n    df = spark.read.parquet(in_file.format(year, month))\n    return df"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "# Read data from a specific day,month,year\n\ndef read_in_data2(in_file, year = None, month = None, day = None):\n    df = spark.read.parquet(in_file.format(year, month, day))\n    return df"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "def downsampling(dataset, n_majority):\n    dataset_major = dataset[(dataset['HIGHER_BUNDLE'] == 0)]\n    dataset_minor = dataset[(dataset['HIGHER_BUNDLE'] == 1)]\n    major = dataset_major.sample(n=n_majority, random_state=10) #downsampling\n    df_sampled = pd.concat([major, dataset_minor])\n    df_shuffled = df_sampled.sample(frac=1)\n    \n    return df_shuffled"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "def up_down_sampling(X, y, up_proportion, down_proportion):\n    # PIPELINE\n    # upsampling\n    over = SMOTE(up_proportion) # , k_neighbors=neighbors\n    # downsampling\n    under = RandomUnderSampler(down_proportion)\n    #under = TomekLinks(ratio=down_proportion)\n    steps = [('o', over), ('u', under)]\n    pipeline = Pipeline(steps=steps)\n    X_resampled, y_resampled = pipeline.fit_resample(X, y)\n    \n    return X_resampled, y_resampled"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "def stratified_sampling(dataset, n_majority):\n    dataset_major = dataset[(dataset['HIGHER_BUNDLE'] == 0)]\n    dataset_minor = dataset[(dataset['HIGHER_BUNDLE'] == 1)]\n    major = dataset_major.groupby('TARIFF_PLAN', group_keys=False).apply(lambda x: x.sample(n_majority,replace=True))\n    df_sampled = pd.concat([major, dataset_minor])\n    df_shuffled = df_sampled.sample(frac=1)\n    \n    return df_shuffled"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "def split_x_y(df):\n    X = df.drop([\"MSISDN\",\"HIGHER_BUNDLE\"], axis=1)\n    y = df[\"HIGHER_BUNDLE\"].values\n    \n    return X,y"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": "def encoding_data(X, y):\n    le = LabelEncoder()\n    objects = X.select_dtypes(include=['object'])\n    object_names = list(objects.columns.values)\n    \n    # Encoding all the categorical columns\n    for object_name in object_names:\n        if (X[object_name].nunique() > 2):\n            enc_pc = pd.get_dummies(X[object_name], drop_first = True)\n            X = X.drop(object_name, axis = 1)\n            X = pd.concat([X,enc_pc], axis = 1)\n        elif (X[object_name].nunique() <= 2):\n            le.fit(X[object_name].astype(str))\n            X[object_name] = le.transform(X[object_name].astype(str))\n\n\n    y = le.fit_transform(y)\n    \n    return X,y"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "def estimate_proportion(y):\n    count_train_0= (y == 0).sum()\n    count_train_1= (y == 1).sum()\n    \n    proportion = float(float(count_train_0) / float(count_train_1))\n    \n    return proportion"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "def split_train_test(X,y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n    \n    return X_train, X_test, y_train, y_test"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": "def evaluation(X_test, y_test, classifier):\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred)\n    accuracy = accuracy_score(y_test, y_pred)\n    \n    target_names = ['No higher bundle', 'Higher Bundle']\n    evaluation_report = classification_report(y_test, y_pred, target_names=target_names)\n    \n    return cm, accuracy, evaluation_report"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "def feature_importance(X_test, classifier):\n    explainer = shap.TreeExplainer(classifier)\n    shap_values = explainer.shap_values(X_test)\n    \n    return shap_values"}, {"cell_type": "markdown", "metadata": {}, "source": "### DF1 <a class=\"anchor\" id=\"df1\"></a>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Higher_Bundle_Migrators"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": "#print(month_val[2], month_val[1])"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}'\n\ny_prev = read_in_data2(status_service, year = year_val[2], month = month_val[2]).select(\"MSISDN\", \"SERVICE_CODE\")\ny_next = read_in_data2(status_service, year = year_val[1], month = month_val[1]).select(\"MSISDN\", \"SERVICE_CODE\")"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "y_prev.createOrReplaceTempView(\"y_prev_view\")\ny_next.createOrReplaceTempView(\"y_next_view\")"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [], "source": "y_prev = spark.sql(\"\"\"SELECT MSISDN, SERVICE_CODE FROM y_prev_view\n                WHERE (SERVICE_CODE=\"BDLEthnicDataH\") OR (SERVICE_CODE=\"BDLDataTazaINT\") \n                \n                OR (SERVICE_CODE=\"BDLIntegLPak\") OR (SERVICE_CODE=\"BDLIntegPak\") \n                OR (SERVICE_CODE=\"BDLIntegLInd\") OR (SERVICE_CODE=\"BDLIntegInd\")\n                OR (SERVICE_CODE=\"BDLIntegLBang\") OR (SERVICE_CODE=\"BDLIntegBang\")\n                OR (SERVICE_CODE=\"BDLAlbania\") OR (SERVICE_CODE=\"BDLVFAlbInt\")\n                \n                OR (SERVICE_CODE=\"BDLXNetData\") OR (SERVICE_CODE=\"BDLPreCombo\")\n                \n                OR (SERVICE_CODE=\"BDLTalkText600\") OR (SERVICE_CODE=\"BDLComboMax\") OR (SERVICE_CODE=\"BDLCUComboXL\")                \n                OR (SERVICE_CODE=\"BDLPasoComboH\") OR (SERVICE_CODE=\"BDLPasoComboXL\") OR (SERVICE_CODE=\"BDLPasoComboML\")\n                OR (SERVICE_CODE=\"BDLPasoComboTL\")\n                \"\"\")"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "y_next = spark.sql(\"\"\"SELECT MSISDN, SERVICE_CODE FROM y_next_view\n                WHERE (SERVICE_CODE=\"BDLEthnicDataH\") OR (SERVICE_CODE=\"BDLDataTazaINT\") \n                \n                OR (SERVICE_CODE=\"BDLIntegLPak\") OR (SERVICE_CODE=\"BDLIntegPak\") \n                OR (SERVICE_CODE=\"BDLIntegLInd\") OR (SERVICE_CODE=\"BDLIntegInd\")\n                OR (SERVICE_CODE=\"BDLIntegLBang\") OR (SERVICE_CODE=\"BDLIntegBang\")\n                OR (SERVICE_CODE=\"BDLAlbania\") OR (SERVICE_CODE=\"BDLVFAlbInt\")\n                \n                OR (SERVICE_CODE=\"BDLXNetData\") OR (SERVICE_CODE=\"BDLPreCombo\")\n                \n                OR (SERVICE_CODE=\"BDLTalkText600\") OR (SERVICE_CODE=\"BDLComboMax\") OR (SERVICE_CODE=\"BDLCUComboXL\")                \n                OR (SERVICE_CODE=\"BDLPasoComboH\") OR (SERVICE_CODE=\"BDLPasoComboXL\") OR (SERVICE_CODE=\"BDLPasoComboML\")\n                OR (SERVICE_CODE=\"BDLPasoComboTL\")\n                \"\"\")"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [], "source": "                # CU\ny_prev = y_prev.withColumn(\"PRICE\", when(col(\"SERVICE_CODE\") == \"BDLTalkText600\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLComboMax\", 13.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLCUComboXL\", 15)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboH\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboXL\", 10)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboML\", 12)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboTL\", 17.5)\n                # TAZA\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLPak\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLInd\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLBang\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLAlbania\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegPak\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegInd\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegBang\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLVFAlbInt\", 8.5)\n                # INTERNATIONAL\n                .when(col(\"SERVICE_CODE\") == \"BDLEthnicDataH\", 8.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLDataTazaINT\", 10.5)\n                # VFPP\n                .when(col(\"SERVICE_CODE\") == \"BDLXNetData\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLPreCombo\", 13.5)\n                )"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [], "source": "                # CU\ny_next = y_next.withColumn(\"PRICE\", when(col(\"SERVICE_CODE\") == \"BDLTalkText600\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLComboMax\", 13.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLCUComboXL\", 15)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboH\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboXL\", 10)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboML\", 12)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboTL\", 17.5)\n                # TAZA\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLPak\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLInd\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLBang\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLAlbania\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegPak\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegInd\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegBang\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLVFAlbInt\", 8.5)\n                # INTERNATIONAL\n                .when(col(\"SERVICE_CODE\") == \"BDLEthnicDataH\", 8.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLDataTazaINT\", 10.5)\n                # VFPP\n                .when(col(\"SERVICE_CODE\") == \"BDLXNetData\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLPreCombo\", 13.5)\n                )"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep the highest value bundle\ny_prev = y_prev.select(\"MSISDN\", \"SERVICE_CODE\", \"PRICE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"PRICE\"))).alias(\"row_num\"))\ny_prev = y_prev.filter(col(\"row_num\") == 1).drop(\"row_num\")\n\n### drop duplicates and keep the highest value bundle\ny_next = y_next.select(\"MSISDN\", \"SERVICE_CODE\", \"PRICE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"PRICE\"))).alias(\"row_num\"))\ny_next = y_next.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": "# Rename columns for next month\ny_next = y_next.withColumnRenamed(\"PRICE\", \"PRICE_NEW\")\ny_next = y_next.withColumnRenamed(\"SERVICE_CODE\", \"SERVICE_CODE_NEW\")"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": "y_prev.createOrReplaceTempView(\"y_prev_view\")\ny_next.createOrReplaceTempView(\"y_next_view\")"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": "migrations = spark.sql(\"\"\"SELECT A.MSISDN, A.PRICE, B.PRICE_NEW\n                                FROM y_prev_view A\n                                LEFT JOIN y_next_view B\n                                ON A.MSISDN= B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "migrations = migrations.na.fill(value= 0, subset=[\"PRICE_NEW\"])"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": "# ADD NEW COLUMN FLAG FOR THOSE WHO PURCHASED HIGHER VALUE BUNDLE THE NEXT MONTH\nmigrations = migrations.withColumn(\"HIGHER_BUNDLE\", when(col(\"PRICE\") < col(\"PRICE_NEW\"), 1).otherwise(0))"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": "higher_bundle_segmentation_1 = migrations.select(\"MSISDN\", \"HIGHER_BUNDLE\")"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [], "source": "#higher_bundle_segmentation_1.groupby(\"HIGHER_BUNDLE\").count().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Train Base"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [], "source": "#print(year_val[2], month_val[2])"}, {"cell_type": "markdown", "metadata": {}, "source": "### gdpr"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [], "source": "day_max = 1\ngdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(gdpr, year = year_val[2], month = month_val[2]).select('MSISDN_CLI', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [], "source": "gdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/day={}/'\n\ndf_gdpr = read_in_data2(gdpr, year = year_val[2], month = month_val[2], day=day_max)"}, {"cell_type": "markdown", "metadata": {}, "source": "### status"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [], "source": "# find max day\nday_max = 1\nstatus = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/'\n\n#print(year_val[2], month_val[2])\nlocals()[\"find_day\"] = read_in_data2(status, year = year_val[2], month = month_val[2]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [], "source": "status = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/day={}/'\n\ndf_status = read_in_data2(status, year = year_val[2], month = month_val[2], day= day_max).select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [], "source": "#df_status = df_status.select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": "df_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [], "source": "df_status = df_status.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"data": {"text/plain": "2663296"}, "execution_count": 47, "metadata": {}, "output_type": "execute_result"}], "source": "df_status.count()"}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [], "source": "test = spark.sql(\"\"\"SELECT *\n                         FROM (\n                             SELECT substring(MSISDN, 3 , 10) AS MSISDN, TARIFF_PLAN, CONNECTION_DAY, SMARTPHONE_FLAG, INSERTED\n                             FROM status_view \n                             WHERE STATUS IN ('A','B'))\n                            \"\"\" )"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"data": {"text/plain": "2487966"}, "execution_count": 49, "metadata": {}, "output_type": "execute_result"}], "source": "test.count()"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": "df_gdpr.createOrReplaceTempView(\"gdpr_view\")\ndf_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [], "source": "# inner join status with gdpr\nconsent_prepay_status = spark.sql(\"\"\"SELECT A.*\n                         FROM (\n                             SELECT substring(MSISDN, 3 , 10) AS MSISDN, TARIFF_PLAN, CONNECTION_DAY, SMARTPHONE_FLAG, INSERTED\n                             FROM status_view A\n                             WHERE STATUS IN ('A','B')\n                             ) AS A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\")"}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"data": {"text/plain": "2481657"}, "execution_count": 52, "metadata": {}, "output_type": "execute_result"}], "source": "consent_prepay_status.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "### usage"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [], "source": "# print(month_val[2], year_val[2])\n# print(month_val[3], year_val[3])\n# print(month_val[4], year_val[4])\n# print(month_val[5], year_val[5])\n# print(month_val[6], year_val[6])\n# print(month_val[7], year_val[7])"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [], "source": "# find max day available in dataset for each month\nk = 0\nday_max = []\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[2:],month_val[2:]):\n    k=k+1\n    locals()[\"find_day\"+str(k)] = read_in_data2(usage, year = i, month = j).select('MSISDN', 'day')\n    locals()[\"day_max\"+str(k)] = locals()[\"find_day\"+str(k)].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\n    day_max.append(locals()[\"day_max\"+str(k)])"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [], "source": "# Read datasets from the previous 6-month period\nk = 0 \nday = 1\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/day={}/'\n\nfor i,j,m in zip(year_val[2:], month_val[2:], day_max):\n    k=k+1\n    locals()[\"usage_m\"+str(k)] = read_in_data2(usage, year = i, month = j, day = m)"}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [], "source": "for name in usage_m1.schema.names: usage_m1 = usage_m1.withColumnRenamed(name, name.replace('M1', 'M1'))\nfor name in usage_m2.schema.names: usage_m2 = usage_m2.withColumnRenamed(name, name.replace('M1', 'M2'))\nfor name in usage_m3.schema.names: usage_m3 = usage_m3.withColumnRenamed(name, name.replace('M1', 'M3'))\nfor name in usage_m4.schema.names: usage_m4 = usage_m4.withColumnRenamed(name, name.replace('M1', 'M4'))\nfor name in usage_m5.schema.names: usage_m5 = usage_m5.withColumnRenamed(name, name.replace('M1', 'M5'))\nfor name in usage_m6.schema.names: usage_m6 = usage_m6.withColumnRenamed(name, name.replace('M1', 'M6'))"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [], "source": "usage_m2 = usage_m2.withColumnRenamed(\"MSISDN\", \"M2_MSISDN\")\nusage_m3 = usage_m3.withColumnRenamed(\"MSISDN\", \"M3_MSISDN\")\nusage_m4 = usage_m4.withColumnRenamed(\"MSISDN\", \"M4_MSISDN\")\nusage_m5 = usage_m5.withColumnRenamed(\"MSISDN\", \"M5_MSISDN\")\nusage_m6 = usage_m6.withColumnRenamed(\"MSISDN\", \"M6_MSISDN\")"}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [], "source": "usage_m1.createOrReplaceTempView(\"usage_view_m1\")\nusage_m2.createOrReplaceTempView(\"usage_view_m2\")\nusage_m3.createOrReplaceTempView(\"usage_view_m3\")\nusage_m4.createOrReplaceTempView(\"usage_view_m4\")\nusage_m5.createOrReplaceTempView(\"usage_view_m5\")\nusage_m6.createOrReplaceTempView(\"usage_view_m6\")"}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [], "source": "# Take active base for the last three months\nusage_prepay = spark.sql(\"\"\"SELECT substring(A.MSISDN, 3 , 10) AS MSISDN,\n                     A.M1_TTL_OUT_CALLS, A.M1_TTL_OUT_MINUTES,\n                     A.M1_TTL_OUT_REVENUE, A.M1_V_CALLS_TO_FIXED_LINES, \n                     A.M1_V_MINUTES_TO_FIXED_LINES, A.M1_V_REVENUE_TO_FIXED_LINES, \n                     A.M1_V_CALLS_TO_COMPETITION, A.M1_V_MINUTES_TO_COMPETITION, A.M1_V_REVENUE_TO_COMPETITION,\n                     A.M1_V_CALLS_TO_INTERNATIONAL, A.M1_V_MINUTES_TO_INTERNATIONAL, \n                     A.M1_V_REVENUE_TO_INTERNATIONAL, A.M1_GPRS_SESSION,\n                     A.M1_GPRS_VOLUME, A.M1_GPRS_REVENUE, A.M1_RECHARGES_NUMBER, \n                     A.M1_RECHARGES_VALUE, A.M1_OUT_DAYS, A.M1_INC_DAYS, A.M1_BUNDLE_REVENUE,\n                     \n                     B.M2_TTL_OUT_CALLS, B.M2_TTL_OUT_MINUTES, \n                     B.M2_TTL_OUT_REVENUE, B.M2_V_CALLS_TO_FIXED_LINES,\n                     B.M2_V_MINUTES_TO_FIXED_LINES, B.M2_V_REVENUE_TO_FIXED_LINES, \n                     B.M2_V_CALLS_TO_COMPETITION, B.M2_V_MINUTES_TO_COMPETITION, \n                     B.M2_V_REVENUE_TO_COMPETITION, B.M2_V_CALLS_TO_INTERNATIONAL,\n                     B.M2_V_MINUTES_TO_INTERNATIONAL, B.M2_V_REVENUE_TO_INTERNATIONAL, \n                     B.M2_GPRS_SESSION, B.M2_GPRS_VOLUME, \n                     B.M2_GPRS_REVENUE, B.M2_RECHARGES_NUMBER, \n                     B.M2_RECHARGES_VALUE, B.M2_OUT_DAYS,\n                     B.M2_INC_DAYS, B.M2_BUNDLE_REVENUE,\n                     \n                     C.M3_TTL_OUT_CALLS, C.M3_TTL_OUT_MINUTES, \n                     C.M3_TTL_OUT_REVENUE, C.M3_V_CALLS_TO_FIXED_LINES,\n                     C.M3_V_MINUTES_TO_FIXED_LINES, C.M3_V_REVENUE_TO_FIXED_LINES, \n                     C.M3_V_CALLS_TO_COMPETITION, C.M3_V_MINUTES_TO_COMPETITION, \n                     C.M3_V_REVENUE_TO_COMPETITION, C.M3_V_CALLS_TO_INTERNATIONAL,\n                     C.M3_V_MINUTES_TO_INTERNATIONAL, C.M3_V_REVENUE_TO_INTERNATIONAL, \n                     C.M3_GPRS_SESSION, C.M3_GPRS_VOLUME, \n                     C.M3_GPRS_REVENUE, C.M3_RECHARGES_NUMBER, \n                     C.M3_RECHARGES_VALUE, C.M3_OUT_DAYS,\n                     C.M3_INC_DAYS, C.M3_BUNDLE_REVENUE,\n                     \n                     D.M4_TTL_OUT_CALLS, D.M4_TTL_OUT_MINUTES, \n                     D.M4_TTL_OUT_REVENUE, D.M4_V_CALLS_TO_FIXED_LINES,\n                     D.M4_V_MINUTES_TO_FIXED_LINES, D.M4_V_REVENUE_TO_FIXED_LINES, \n                     D.M4_V_CALLS_TO_COMPETITION, D.M4_V_MINUTES_TO_COMPETITION, \n                     D.M4_V_REVENUE_TO_COMPETITION, D.M4_V_CALLS_TO_INTERNATIONAL,\n                     D.M4_V_MINUTES_TO_INTERNATIONAL, D.M4_V_REVENUE_TO_INTERNATIONAL, \n                     D.M4_GPRS_SESSION, D.M4_GPRS_VOLUME, \n                     D.M4_GPRS_REVENUE, D.M4_RECHARGES_NUMBER, \n                     D.M4_RECHARGES_VALUE, D.M4_OUT_DAYS, \n                     D.M4_INC_DAYS, D.M4_BUNDLE_REVENUE,\n                     \n                     E.M5_TTL_OUT_CALLS, E.M5_TTL_OUT_MINUTES, \n                     E.M5_TTL_OUT_REVENUE, E.M5_V_CALLS_TO_FIXED_LINES,\n                     E.M5_V_MINUTES_TO_FIXED_LINES, E.M5_V_REVENUE_TO_FIXED_LINES, \n                     E.M5_V_CALLS_TO_COMPETITION, E.M5_V_MINUTES_TO_COMPETITION, \n                     E.M5_V_REVENUE_TO_COMPETITION, E.M5_V_CALLS_TO_INTERNATIONAL,\n                     E.M5_V_MINUTES_TO_INTERNATIONAL, E.M5_V_REVENUE_TO_INTERNATIONAL, \n                     E.M5_GPRS_SESSION, E.M5_GPRS_VOLUME, \n                     E.M5_GPRS_REVENUE, E.M5_RECHARGES_NUMBER, \n                     E.M5_RECHARGES_VALUE, E.M5_OUT_DAYS,\n                     E.M5_INC_DAYS, E.M5_BUNDLE_REVENUE,\n                  \n                     F.M6_TTL_OUT_CALLS, F.M6_TTL_OUT_MINUTES, \n                     F.M6_TTL_OUT_REVENUE, F.M6_V_CALLS_TO_FIXED_LINES,\n                     F.M6_V_MINUTES_TO_FIXED_LINES, F.M6_V_REVENUE_TO_FIXED_LINES, \n                     F.M6_V_CALLS_TO_COMPETITION, F.M6_V_MINUTES_TO_COMPETITION, \n                     F.M6_V_REVENUE_TO_COMPETITION, F.M6_V_CALLS_TO_INTERNATIONAL,\n                     F.M6_V_MINUTES_TO_INTERNATIONAL, F.M6_V_REVENUE_TO_INTERNATIONAL, \n                     F.M6_GPRS_SESSION, F.M6_GPRS_VOLUME, \n                     F.M6_GPRS_REVENUE, F.M6_RECHARGES_NUMBER, \n                     F.M6_RECHARGES_VALUE, F.M6_OUT_DAYS,\n                     F.M6_INC_DAYS, F.M6_BUNDLE_REVENUE\n                     \n                     from usage_view_m1 A\n                     left join usage_view_m2 B\n                          on A.MSISDN = B.M2_MSISDN \n                     left join usage_view_m3 C\n                          on A.MSISDN = C.M3_MSISDN\n                     left join usage_view_m4 D\n                          on A.MSISDN = D.M4_MSISDN\n                     left join usage_view_m5 E\n                          on A.MSISDN = E.M5_MSISDN\n                     left join usage_view_m6 F\n                          on A.MSISDN = F.M6_MSISDN\n                          \n                     WHERE ((A.M1_TTL_OUT_CALLS > 0) or (A.M1_GPRS_SESSION > 0))\"\"\") "}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [], "source": "usage_prepay = usage_prepay.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [], "source": "usage_prepay.createOrReplaceTempView(\"usage_prepay_view\")\ndf_gdpr.createOrReplaceTempView(\"gdpr_view\")"}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [], "source": "# join usage with consent\nconsent_prepay_usage = spark.sql(\"\"\"SELECT A.*\n                         FROM usage_prepay_view A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\") "}, {"cell_type": "markdown", "metadata": {}, "source": "# join status with usage"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": "consent_prepay_usage.createOrReplaceTempView(\"consent_prepay_usage_view\")\nconsent_prepay_status.createOrReplaceTempView(\"consent_prepay_status_view\")"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [], "source": "base_usage_status = spark.sql(\"\"\"SELECT A.*, B.TARIFF_PLAN, B.CONNECTION_DAY, B.SMARTPHONE_FLAG, B.INSERTED\n                             FROM consent_prepay_usage_view A\n                             INNER JOIN consent_prepay_status_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])\n    elif (dict(base_usage_status.dtypes)[c] == 'object' or dict(base_usage_status.dtypes)[c] == 'string' or dict(base_usage_status.dtypes)[c] == 'timestamp'):\n        base_usage_status = base_usage_status.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### Add/Convert_Features"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [], "source": "# 1. Convert from second -> minutes\n# 2. Convert from KByte -> MByte\nfor column in base_usage_status.columns:\n    if 'MINUTES' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/60)\n    if 'VOLUME' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/1024)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Group TARIFFS"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [], "source": "# (ETHNIC -> INT, TAZA) & (DOMESTIC -> CU, VFPP)"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [], "source": "# CU\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba40', 'CU'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba', 'CU'))"}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [], "source": "# VFPP\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'VALCBASE', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'HAM', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Advanced', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'OCFP', 'VFPP'))"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [], "source": "# ETHNIC (INTERNATIONAL + TAZA)\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'INTPACK', 'INTERNATIONAL'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Taza', 'TAZA'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'TAZA', 'TAZA'))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Tenure"}, {"cell_type": "code", "execution_count": 71, "metadata": {}, "outputs": [], "source": "# Calculate tenure in months\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", round(F.months_between(col(\"INSERTED\"), col(\"CONNECTION_DAY\"))))\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", col(\"TENURE_IN_MONTHS\").cast(IntegerType()))"}, {"cell_type": "code", "execution_count": 72, "metadata": {}, "outputs": [], "source": "# SELECT ONLY CUSTOMERS THAT ARE MORE THAN 3 MONTHS IN OUR DATABASE\nbase_usage_status = base_usage_status[base_usage_status[\"TENURE_IN_MONTHS\"] > 3] "}, {"cell_type": "markdown", "metadata": {}, "source": "# Average talk per time"}, {"cell_type": "code", "execution_count": 73, "metadata": {}, "outputs": [], "source": "for month in range(1,7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+ str(month) + \"_MINUTES_PER_CALL\", \n                                                     col(\"M\" + str(month) + \"_TTL_OUT_MINUTES\") / col(\"M\" + str(month) + \"_TTL_OUT_CALLS\"))"}, {"cell_type": "code", "execution_count": 74, "metadata": {}, "outputs": [], "source": "# Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# ARPU"}, {"cell_type": "code", "execution_count": 75, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+\"_ARPU\", \n                        col(\"M\"+str(month)+\"_TTL_OUT_REVENUE\") + col(\"M\"+str(month)+\"_GPRS_REVENUE\") \n                                                     + col(\"M\"+str(month)+\"_BUNDLE_REVENUE\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# ROC"}, {"cell_type": "code", "execution_count": 76, "metadata": {}, "outputs": [], "source": "for column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        prev_month_avg = (col(\"M2\"+column[2:]) + col(\"M3\"+column[2:]) + col(\"M4\"+column[2:]) + col(\"M5\"+column[2:]) + col(\"M6\"+column[2:])) / 5 \n        base_usage_status = base_usage_status.withColumn(\"M1_ROC\"+column[2:], (col(\"M1\"+column[2:]) -  prev_month_avg) / prev_month_avg )"}, {"cell_type": "code", "execution_count": 77, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Average for all and half period"}, {"cell_type": "code", "execution_count": 78, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\nfor column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        # first semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M13_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:]))/3)\n        # second semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M46_AVG\"+ column[2:], (col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:]))/3)\n        # calculate the average for all six months     \n        base_usage_status = base_usage_status.withColumn(\"M16_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:])+col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:])) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Ratio (recharge value / bundle value)"}, {"cell_type": "code", "execution_count": 79, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+ \"_EXPENDITURE_RATIO\",\n                                                     col(\"M\"+str(month)+ \"_RECHARGES_VALUE\") / col(\"M\"+str(month)+ \"_BUNDLE_REVENUE\"))"}, {"cell_type": "code", "execution_count": 80, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### demographics"}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [], "source": "# CAR LINE DATASET\nline = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(line, year = year_val[2], month = month_val[2]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": 82, "metadata": {}, "outputs": [], "source": "line = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/day={}/'\n\ndf_line = read_in_data2(line, year = year_val[2], month = month_val[2], day= day_max).select(\"MSISDN\", \"RETAIL_CUST_ACCT_DWH_ID\")\n#df_line = df_line.select(\"MSISDN\", \"RETAIL_CUST_ACCT_DWH_ID\")"}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\",\"RETAIL_CUST_ACCT_DWH_ID\"])"}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [], "source": "base_usage_status.createOrReplaceTempView(\"base_usage_view\")\ndf_line.createOrReplaceTempView(\"line_view\")"}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [], "source": "# Join CAR_LINE with BASE_USAGE\nusage_status_df = spark.sql (\"\"\"SELECT A.*, B.RETAIL_CUST_ACCT_DWH_ID\n                          FROM base_usage_view A\n                          INNER JOIN line_view B\n                             ON A.MSISDN = B.MSISDN\n                       \"\"\")"}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [], "source": "# DEMOGRAPHICS DATASET\ndemographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(demographics, year = year_val[2], month = month_val[2]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": 88, "metadata": {}, "outputs": [], "source": "demographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/day={}/'\n\ndf_demographics = read_in_data2(demographics, year = year_val[2], month = month_val[2], day= day_max)"}, {"cell_type": "code", "execution_count": 89, "metadata": {}, "outputs": [], "source": "# select specific columns\ndf_demographics = df_demographics.select(\"CUST_DWH_ID\", \"POST_CODE\", \"GENDER\", \"AGE\", \"VF_COMBO_FLG\", \"ACTIVE_TOTAL_LINES\")"}, {"cell_type": "code", "execution_count": 90, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.dropDuplicates([\"CUST_DWH_ID\"])"}, {"cell_type": "code", "execution_count": 91, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn('POST_CODE', regexp_replace('POST_CODE', 'XXXXX', 'DUMMY'))"}, {"cell_type": "code", "execution_count": 92, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if (dict(df_demographics.dtypes)[column] == 'int64' or dict(df_demographics.dtypes)[column] == 'double' or dict(df_demographics.dtypes)[column] == 'int'):\n        # fill with mean\n        mean = df_demographics.agg({column: \"avg\"}).collect()[0][0]\n        df_demographics = df_demographics.na.fill(mean, subset=[column])\n    elif (dict(df_demographics.dtypes)[column] == 'object' or dict(df_demographics.dtypes)[column] == 'string'):\n        if (column == \"GENDER\"):\n            df_demographics = df_demographics.na.fill(value=\"O\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n        if (column == \"POST_CODE\"):\n            df_demographics = df_demographics.na.fill(value=\"DUMMY\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n        df_demographics = df_demographics.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "code", "execution_count": 93, "metadata": {}, "outputs": [], "source": "# STRANGE VALUES FOR AGES\ndf_demographics = df_demographics.withColumn(\"AGE\", coalesce(col(\"AGE\"), lit(0.0)))"}, {"cell_type": "code", "execution_count": 94, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") > 80, 80).otherwise(col(\"AGE\")))\ndf_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") < 17, 18).otherwise(col(\"AGE\")))"}, {"cell_type": "markdown", "metadata": {}, "source": "# JOIN USAGE STATUS WITH DEMOGRAPHICS"}, {"cell_type": "code", "execution_count": 95, "metadata": {}, "outputs": [], "source": "df_demographics.createOrReplaceTempView(\"demographics_view\")\nusage_status_df.createOrReplaceTempView(\"usage_status_view\")"}, {"cell_type": "code", "execution_count": 96, "metadata": {}, "outputs": [], "source": "# JOIN\nusage_status_demo = spark.sql (\"\"\"SELECT A.*, B.POST_CODE, B.GENDER, B.AGE, B.VF_COMBO_FLG, B.ACTIVE_TOTAL_LINES\n                          FROM usage_status_view A\n                          LEFT JOIN demographics_view B\n                             ON A.RETAIL_CUST_ACCT_DWH_ID = B.CUST_DWH_ID\n                       \"\"\")"}, {"cell_type": "code", "execution_count": 97, "metadata": {}, "outputs": [], "source": "usage_status_demo = usage_status_demo.drop(col(\"RETAIL_CUST_ACCT_DWH_ID\"))"}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if column != 'CUST_DWH_ID':\n        if (dict(usage_status_demo.dtypes)[column] == 'int64' or dict(usage_status_demo.dtypes)[column] == 'double' or dict(usage_status_demo.dtypes)[column] == 'int'):\n            # fill with mean\n            mean = usage_status_demo.agg({column: \"avg\"}).collect()[0][0]\n            usage_status_demo = usage_status_demo.na.fill(mean, subset=[column])\n        elif (dict(usage_status_demo.dtypes)[column] == 'object' or dict(usage_status_demo.dtypes)[column] == 'string'):\n            if (column == \"GENDER\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"O\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n            if (column == \"POST_CODE\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"DUMMY\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n            usage_status_demo = usage_status_demo.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### post_code"}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [{"data": {"text/plain": "0"}, "execution_count": 99, "metadata": {}, "output_type": "execute_result"}], "source": "subprocess.call('/bin/sh /usr/bin/gsutil -q cp gs://' + files_bucket + '/notebooks/jupyter/higher_bundles/Sociodemographics.xlsx Sociodemographics.xlsx', shell=True)\nPopulation_pools = pd.read_excel('Sociodemographics.xlsx')\npopulation_pools_df = sql.createDataFrame(Population_pools)"}, {"cell_type": "code", "execution_count": 100, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.drop(\"Postcode_key\", \"Name\", \"Periferiaki_enotita\", \"Population_aged_60+\",\n                                              \"Male_Population_aged_60+\", \"Female_Population_aged_60+\")"}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.withColumnRenamed('Population_aged_0-14', 'Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_0-14', 'Male_Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_0-14', 'Female_Population_aged_0_14')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_15-29', 'Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_15-29', 'Male_Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_15-29', 'Female_Population_aged_15_29')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_30-44', 'Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_30-44', 'Male_Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_30-44', 'Female_Population_aged_30_44')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_45-59', 'Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_45-59', 'Male_Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_45-59', 'Female_Population_aged_45_59')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_million_Euro', 'Purchasing_Power_million_Euro')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_per_mill_of_country', 'Purchasing_Power_per_mill_of_country')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_Euro_per_capita', 'Purchasing_Power_Euro_per_capita')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_index_(country_eq.100)', 'Purchasing_Power_index_country_eq_100')"}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [], "source": "population_pools_df.createOrReplaceTempView(\"population_pools_view\")\nusage_status_demo.createOrReplaceTempView(\"usage_status_demo_view\")"}, {"cell_type": "code", "execution_count": 103, "metadata": {}, "outputs": [], "source": "# Exclude ages from 60+\nusage_status_demo_pc = spark.sql (\"\"\"SELECT A.*, B.Population, B.Households, B.Average_Household_Size, B.Male_Population,\n                          B.Female_Population, B.Population_aged_0_14, B.Male_Population_aged_0_14, B.Female_Population_aged_0_14,\n                          B.Population_aged_15_29, B.Male_Population_aged_15_29, B.Female_Population_aged_15_29, \n                          B.Population_aged_30_44, B.Male_Population_aged_30_44, B.Female_Population_aged_30_44,\n                          B.Population_aged_45_59, B.Male_Population_aged_45_59, B.Female_Population_aged_45_59,\n                          B.Purchasing_Power_million_Euro, B.Purchasing_Power_per_mill_of_country,\n                          B.Purchasing_Power_Euro_per_capita, B.Purchasing_Power_index_country_eq_100\n\n                          FROM usage_status_demo_view A                          \n                          LEFT JOIN population_pools_view B                          \n                          ON A.POST_CODE = B.POST_CODE\n                       \"\"\")"}, {"cell_type": "code", "execution_count": 104, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in population_pools_df.columns:\n    usage_status_demo_pc = usage_status_demo_pc.na.fill(value=0, subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### students"}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(status_service, year = year_val[2], month = month_val[2]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/day={}'\n\ndf_status_service = read_in_data2(status_service, year = year_val[2], month = month_val[2], day=day_max)"}, {"cell_type": "code", "execution_count": 107, "metadata": {}, "outputs": [], "source": "df_status_service.createOrReplaceTempView(\"df_status_service_view\")"}, {"cell_type": "code", "execution_count": 108, "metadata": {}, "outputs": [], "source": "df_students = spark.sql(\"\"\"SELECT * \n                           FROM df_status_service_view A\n                           WHERE SERVICE_CODE == 'BDLCUPaso' \n                           \"\"\")"}, {"cell_type": "code", "execution_count": 109, "metadata": {}, "outputs": [], "source": "df_students = df_students.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": 110, "metadata": {}, "outputs": [], "source": "# ADD A NEW COLUMN WITH A FLAG TO INIDICATE THAT THIS USER IS STUDENT\ndf_students = df_students.withColumn(\"STUDENTS_FLAG\", lit(\"Y\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# LEFT JOIN USAGE_STATUS_DEMO WITH STUDENTS INFO"}, {"cell_type": "code", "execution_count": 111, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc.createOrReplaceTempView(\"usage_status_demo_pc_view\")\ndf_students.createOrReplaceTempView(\"students_view\")"}, {"cell_type": "code", "execution_count": 112, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud = spark.sql(\"\"\"SELECT A.*, B.STUDENTS_FLAG\n                                FROM usage_status_demo_pc_view A\n                                LEFT JOIN students_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 113, "metadata": {}, "outputs": [], "source": "# SET indicator N for NO if the user is not a students\nusage_status_demo_pc_stud = usage_status_demo_pc_stud.na.fill(value=\"N\", subset=[\"STUDENTS_FLAG\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### buckets"}, {"cell_type": "code", "execution_count": 114, "metadata": {}, "outputs": [{"data": {"text/plain": "[9, 8]"}, "execution_count": 114, "metadata": {}, "output_type": "execute_result"}], "source": "month_val[2:4]"}, {"cell_type": "code", "execution_count": 115, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 2-month period\nk = 0 \nday = 1\nbuckets = 'gs://'+ model_outputs_bucket + '/prepay_buckets/result/parquet/1.0/year={}/month={}/'\n\nfor i,j in zip(year_val[2:4], month_val[2:4]):\n    k=k+1\n    locals()[\"buckets_m\"+str(k)] = read_in_data2(buckets, year = i, month = j)"}, {"cell_type": "code", "execution_count": 116, "metadata": {}, "outputs": [], "source": "df_buckets = buckets_m1.union(buckets_m2)"}, {"cell_type": "code", "execution_count": 117, "metadata": {}, "outputs": [], "source": "df_buckets = df_buckets.sort(col(\"BUNDLE\").asc(), col(\"ACTIVATION_DATE\").desc())"}, {"cell_type": "code", "execution_count": 118, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep first occurrence for each bundle\ndf_buckets = df_buckets.select(\"MSISDN\", \"ACTIVATION_DATE\", \"BUNDLE\", \"BUNDLE_REVENUE\", \"BALANCE\", \"VOICE_BUCKET\", \"DATA_BUCKET\", \"SMS_BUCKET\",\n                                   F.row_number().over(Window.partitionBy(\"MSISDN\", \"BUNDLE\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_buckets = df_buckets.filter(col(\"row_num\") == 1)"}, {"cell_type": "code", "execution_count": 119, "metadata": {}, "outputs": [], "source": "# Calculate summary for all buckets\ndf_buckets_summary = df_buckets.groupBy(\"MSISDN\").agg(sum(\"VOICE_BUCKET\").alias(\"VOICE_BUCKET_SUMMARY\"),sum(\"DATA_BUCKET\").alias(\"DATA_BUCKET_SUMMARY\"),sum(\"SMS_BUCKET\").alias(\"SMS_BUCKET_SUMMARY\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# balance"}, {"cell_type": "code", "execution_count": 120, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep last registration \ndf_balance = df_buckets.select(\"MSISDN\", \"BALANCE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_balance_summary = df_balance.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join buckets with balance"}, {"cell_type": "code", "execution_count": 121, "metadata": {}, "outputs": [], "source": "df_buckets_summary.createOrReplaceTempView(\"buckets_sum_view\")\ndf_balance_summary.createOrReplaceTempView(\"balance_sum_view\")"}, {"cell_type": "code", "execution_count": 122, "metadata": {}, "outputs": [], "source": "df_buckets_balance = spark.sql(\"\"\"SELECT A.*, B.BALANCE\n                             FROM buckets_sum_view A\n                             INNER JOIN balance_sum_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join bucket-balance with main dataset"}, {"cell_type": "code", "execution_count": 123, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud.createOrReplaceTempView(\"usage_status_demo_pc_stud_view\")\ndf_buckets_balance.createOrReplaceTempView(\"buckets_balance_view\")"}, {"cell_type": "code", "execution_count": 124, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets = spark.sql(\"\"\"SELECT A.*, B.VOICE_BUCKET_SUMMARY, B.DATA_BUCKET_SUMMARY, B.SMS_BUCKET_SUMMARY, B.BALANCE\n                                FROM usage_status_demo_pc_stud_view A\n                                LEFT JOIN buckets_balance_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 125, "metadata": {}, "outputs": [], "source": "for c in df_buckets_balance.columns:\n    if (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'int64' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'double' or dict(usage_status_demo_pc_stud_buckets.dtypes)[column] == 'int'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=0, subset=[c])\n    elif (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'object' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'string' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'timestamp'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### drop_calls"}, {"cell_type": "code", "execution_count": 126, "metadata": {}, "outputs": [], "source": "drop_calls = 'gs://'+ mediatedcdrs_bucket +'/eds_network_cdr/2.0/parquet/year={}/month={}/'\n\ndf_drop_calls = read_in_data2(drop_calls, year = year_val[2], month = month_val[2])"}, {"cell_type": "code", "execution_count": 127, "metadata": {}, "outputs": [], "source": "df_drop_calls = df_drop_calls.select(\"SAMPLED\",\"A_NUMBER\",\"FIRST_LAC\",\"LAST_LAC\",\"CELL\",\"LAST_CELL\",\"REC_TYPE\",\"TARIFF\",\n                               \"DURATION\", \"TERM_CAUSE\", \"day\")"}, {"cell_type": "code", "execution_count": 128, "metadata": {}, "outputs": [], "source": "df_drop_calls.createOrReplaceTempView(\"cdrs_view\")"}, {"cell_type": "code", "execution_count": 129, "metadata": {}, "outputs": [], "source": "drop_calls_query = spark.sql(\"\"\"SELECT K.MSISDN,\n                                count(*) as DROPPED_CALL_COUNT\n                                FROM (\n                                    SELECT L.MSISDN, L.SAMPLED, L.YEAR\n                                    FROM (\n                                       SELECT\n                                       A.SAMPLED, A.A_NUMBER AS MSISDN, A.day, YEAR(A.SAMPLED) AS YEAR, SUBSTR(A.TERM_CAUSE,1,4) AS EOS,\n                                       CASE WHEN A.FIRST_LAC LIKE '%IE%' THEN A.LAST_LAC  --this condition holds only for calls\n                                           WHEN A.FIRST_LAC = '' THEN A.LAST_LAC          --this condition holds only for calls\n                                           ELSE A.FIRST_LAC END FIRST_LAC,                --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_LAC = '' THEN A.FIRST_LAC         --this condition holds for both calls and SMS\n                                           ELSE A.LAST_LAC END LAST_LAC,                  --this condition holds for both calls and SMS\n                                       CASE WHEN A.CELL LIKE '%F%' THEN A.LAST_CELL       --this condition holds only for calls\n                                           WHEN A.CELL = '' THEN A.LAST_CELL              --this condition holds only for calls\n                                           ELSE A.CELL END CELL,                          --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_CELL = '' THEN A.CELL             --this condition holds for both calls and SMS\n                                           ELSE A.LAST_CELL END LAST_CELL,                --this condition holds for both calls and SMS\n                                       ROW_NUMBER() OVER (PARTITION BY A.A_NUMBER, A.SAMPLED ORDER BY A.A_NUMBER, A.SAMPLED) as LEVEL\n                                       FROM cdrs_view A\n                                       WHERE ((A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL != '') OR  -- this condition holds only for calls\n                                       (A.FIRST_LAC = '' AND A.LAST_LAC != '' AND A.CELL = '' AND A.LAST_CELL != '') OR    -- this condition holds only for calls\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC = '' AND A.CELL != '' AND A.LAST_CELL = '') OR    -- this condition holds for both calls and SMS\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL = ''))     -- this condition holds for both calls and SMS\n                                       AND A.REC_TYPE IN ('20','30') AND A.TARIFF != '142'\n                                       AND LENGTH(A.TERM_CAUSE) = 8\n                                       AND SUBSTR(A.TERM_CAUSE,1,4) IN ('068F','08BF','09A6','09C3','09C5','09C8','09F8','0A0E','0A0F','0AE9','0C15','0CD2',\n                                                                        '0CD3','0F7B','0F7C','018F','065D','065E','0700','0701','0702','09A7','09BF','09C0',\n                                                                        '09C2','09C4','09C6','09C7','09C9','09F6','09F7','0A0A','0A0B','0A0C','0A0D','0C14',\n                                                                        '0C16','0F7D','1C8F','1C90','1C91','1C92','1C9A','1C9B')\n                                       AND A.A_NUMBER != '' AND A.A_NUMBER LIKE '69%' \n                                       --ORDER BY A.A_NUMBER, A.SAMPLED\n                                       ) AS L\n                                    WHERE L.LEVEL = 1\n                                    --ORDER BY L.MSISDN, L.SAMPLED ASC\n                                ) K\n                                GROUP BY K.MSISDN\n                                ORDER BY K.MSISDN\n                             \"\"\")  "}, {"cell_type": "markdown", "metadata": {}, "source": "# Join drop calls with main dataset"}, {"cell_type": "code", "execution_count": 130, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_view\")\ndrop_calls_query.createOrReplaceTempView(\"drop_calls_view\")"}, {"cell_type": "code", "execution_count": 131, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = spark.sql(\"\"\"SELECT A.*, B.DROPPED_CALL_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_view A\n                                LEFT JOIN drop_calls_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = usage_status_demo_pc_stud_buckets_dropcalls.na.fill(value=0, subset=[\"DROPPED_CALL_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### tickets"}, {"cell_type": "code", "execution_count": 133, "metadata": {}, "outputs": [], "source": "ticket_service = 'gs://'+ dhdwh_bucket +'/mobile_sr_tt/1.0/parquet/year={}/month={}/'\n\ndf_tickets_requests = read_in_data2(ticket_service, year = year_val[2], month = month_val[2]).select(\"X_MSISDN\",\"SR_ID\").drop('service_file_id')"}, {"cell_type": "code", "execution_count": 134, "metadata": {}, "outputs": [], "source": "#df_tickets_requests = df_tickets_requests.select(\"X_MSISDN\",\"SR_ID\")"}, {"cell_type": "code", "execution_count": 135, "metadata": {}, "outputs": [], "source": "# drop duplicates\ndf_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\", \"SR_ID\"])"}, {"cell_type": "code", "execution_count": 136, "metadata": {}, "outputs": [], "source": "# ADD A COLUMN AS TICKETS COUNTER FOR EACH MSISDN\ndf_tickets_requests = df_tickets_requests.select(\"X_MSISDN\", F.count(\"X_MSISDN\").over(Window.partitionBy(\"X_MSISDN\")).alias(\"TICKETS_COUNT\"))"}, {"cell_type": "code", "execution_count": 137, "metadata": {}, "outputs": [], "source": "df_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join tickets with main dataset"}, {"cell_type": "code", "execution_count": 138, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_view\")\ndf_tickets_requests.createOrReplaceTempView(\"tickets_requests_view\")"}, {"cell_type": "code", "execution_count": 139, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = spark.sql(\"\"\"SELECT A.*, B.TICKETS_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_view A\n                                LEFT JOIN tickets_requests_view B\n                                ON A.MSISDN=B.X_MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 140, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = usage_status_demo_pc_stud_buckets_dropcalls_tickets.na.fill(value=0, subset=[\"TICKETS_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### channel"}, {"cell_type": "code", "execution_count": 141, "metadata": {}, "outputs": [], "source": "events = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'\n\ndf_events = read_in_data2(events, year = year_val[2], month = month_val[2])"}, {"cell_type": "code", "execution_count": 142, "metadata": {}, "outputs": [], "source": "df_events.createOrReplaceTempView(\"df_events_view\")"}, {"cell_type": "code", "execution_count": 143, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = spark.sql(\"\"\"SELECT ACCOUNT_ID, REQUESTING_SYSTEM                             \n                             FROM df_events_view \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             \"\"\")"}, {"cell_type": "code", "execution_count": 144, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'VOP', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUapp', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'MCare', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUsite', 'DIGITAL'))"}, {"cell_type": "code", "execution_count": 145, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'TAZAAPP', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'EKIOSK', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PostpaidToPrepaid', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'XPCVM', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'LMG', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PEGA', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'SMSVAS', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CRM', 'OTHER'))"}, {"cell_type": "code", "execution_count": 146, "metadata": {}, "outputs": [], "source": "# MIA KATHGORIA VFSHOP - (VFSHOP)\n# ALLH KATHGORIA - THLEFWNO (IVR)  \n# DIGITAL (VOP, MCARE, CUapp, CUsite)\n# OTHER"}, {"cell_type": "code", "execution_count": 147, "metadata": {}, "outputs": [], "source": "# One-hot encoding - 4 kathgories\ndf_bundle_purchase = df_bundle_purchase.groupBy('ACCOUNT_ID').pivot('REQUESTING_SYSTEM').count()"}, {"cell_type": "code", "execution_count": 148, "metadata": {}, "outputs": [], "source": "for c in df_bundle_purchase.columns:\n        df_bundle_purchase = df_bundle_purchase.na.fill(0, subset=[c])"}, {"cell_type": "code", "execution_count": 149, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = df_bundle_purchase.select(\"ACCOUNT_ID\", \"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join channel with main dataset"}, {"cell_type": "code", "execution_count": 150, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_view\")\ndf_bundle_purchase.createOrReplaceTempView(\"bundle_purchase_view\")"}, {"cell_type": "code", "execution_count": 151, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = spark.sql(\"\"\"SELECT A.*, B.DIGITAL, B.VFShop, B.IVR, B.OTHER\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_view A\n                                LEFT JOIN bundle_purchase_view B\n                                ON A.MSISDN= SUBSTR(B.ACCOUNT_ID,3,10)\"\"\")"}, {"cell_type": "code", "execution_count": 152, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.na.fill(value=0, subset=[\"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### NumberOfBundles"}, {"cell_type": "code", "execution_count": 153, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 6-month period\nk = 0 \nday = 1\nevents = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[2:], month_val[2:]):\n    k=k+1\n    locals()[\"events_m\"+str(k)] = read_in_data2(events, year = i, month = j).select(\"ACCOUNT_ID\", \"PTP_COSP_AMA_CODE\", \"EVENT_LABEL\", \"EVENT_RESULT\")"}, {"cell_type": "code", "execution_count": 154, "metadata": {}, "outputs": [], "source": "events_m1.createOrReplaceTempView(\"events_view_m1\")\nevents_m2.createOrReplaceTempView(\"events_view_m2\")\nevents_m3.createOrReplaceTempView(\"events_view_m3\")\nevents_m4.createOrReplaceTempView(\"events_view_m4\")\nevents_m5.createOrReplaceTempView(\"events_view_m5\")\nevents_m6.createOrReplaceTempView(\"events_view_m6\")"}, {"cell_type": "code", "execution_count": 155, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m1 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 156, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m2 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m2 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 157, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m3 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m3 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 158, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m4 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m4 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 159, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m5 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m5 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 160, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m6 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m6 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": 161, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1.createOrReplaceTempView(\"num_of_bundles_view_m1\")\ndf_num_of_bundles_m2.createOrReplaceTempView(\"num_of_bundles_view_m2\")\ndf_num_of_bundles_m3.createOrReplaceTempView(\"num_of_bundles_view_m3\")\ndf_num_of_bundles_m4.createOrReplaceTempView(\"num_of_bundles_view_m4\")\ndf_num_of_bundles_m5.createOrReplaceTempView(\"num_of_bundles_view_m5\")\ndf_num_of_bundles_m6.createOrReplaceTempView(\"num_of_bundles_view_m6\")"}, {"cell_type": "code", "execution_count": 162, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = spark.sql(\"\"\"SELECT substring(A.ACCOUNT_ID, 3 , 10) AS ACCOUNT_ID, A.BUNDLES_NUM AS M1_BUNDLES_NUM, \n                     B.BUNDLES_NUM AS M2_BUNDLES_NUM, C.BUNDLES_NUM AS M3_BUNDLES_NUM, \n                     D.BUNDLES_NUM AS M4_BUNDLES_NUM,\n                     E.BUNDLES_NUM AS M5_BUNDLES_NUM, \n                     F.BUNDLES_NUM AS M6_BUNDLES_NUM\n                     \n                     FROM num_of_bundles_view_m1 A                 \n                    \n                     left join num_of_bundles_view_m2 B\n                          on A.ACCOUNT_ID = B.ACCOUNT_ID \n                     left join num_of_bundles_view_m3 C \n                          on A.ACCOUNT_ID = C.ACCOUNT_ID\n                     left join num_of_bundles_view_m4 D \n                          on A.ACCOUNT_ID = D.ACCOUNT_ID\n                     left join num_of_bundles_view_m5 E \n                          on A.ACCOUNT_ID = E.ACCOUNT_ID\n                     left join num_of_bundles_view_m6 F\n                          on A.ACCOUNT_ID = F.ACCOUNT_ID\n                          \"\"\")"}, {"cell_type": "code", "execution_count": 163, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = num_of_bundles_semester.na.fill(value=0)"}, {"cell_type": "code", "execution_count": 164, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\n# first semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M13_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\"))/3)\n# second semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M46_AVG_BUNDLES_NUM\", (col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\"))/3)\n# calculate the average for all six months     \nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M16_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\")+col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\")) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join number of bundles with main dataset"}, {"cell_type": "code", "execution_count": 165, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view\")\nnum_of_bundles_semester.createOrReplaceTempView(\"num_of_bundles_semester_view\")"}, {"cell_type": "code", "execution_count": 166, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = spark.sql(\"\"\"SELECT A.*, B.M1_BUNDLES_NUM, B.M2_BUNDLES_NUM, B.M3_BUNDLES_NUM, \n                                B.M4_BUNDLES_NUM, B.M5_BUNDLES_NUM, B.M6_BUNDLES_NUM, B.M13_AVG_BUNDLES_NUM, B.M46_AVG_BUNDLES_NUM, \n                                B.M16_AVG_BUNDLES_NUM\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view A\n                                LEFT JOIN num_of_bundles_semester_view B\n                                ON A.MSISDN= B.ACCOUNT_ID\"\"\")"}, {"cell_type": "code", "execution_count": 167, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.na.fill(value=0)"}, {"cell_type": "markdown", "metadata": {}, "source": "# JOIN ALL DATASET WITH Y"}, {"cell_type": "code", "execution_count": 168, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.createOrReplaceTempView(\"final_dataset_view\")\nhigher_bundle_segmentation_1.createOrReplaceTempView(\"higher_bundle_segmentation_1_view\")"}, {"cell_type": "code", "execution_count": 169, "metadata": {}, "outputs": [], "source": "final_df_train = spark.sql(\"\"\"SELECT A.*, B.HIGHER_BUNDLE\n                                FROM final_dataset_view A\n                                LEFT JOIN higher_bundle_segmentation_1_view B\n                                ON A.MSISDN= B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": 170, "metadata": {}, "outputs": [], "source": "final_df_train = final_df_train.na.fill(value=0, subset=[\"HIGHER_BUNDLE\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+-------+\n|HIGHER_BUNDLE|  count|\n+-------------+-------+\n|            1|  33988|\n|            0|1286401|\n+-------------+-------+\n\n"}], "source": "final_df_train.groupby(\"HIGHER_BUNDLE\").count().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# ------------------------------------------------------------------------------------------------------------#"}, {"cell_type": "markdown", "metadata": {}, "source": "### DF2 <a class=\"anchor\" id=\"df2\"></a>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Higher_Bundle_Migrators2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print(month_val[1], month_val[0])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\n\ny_prev = read_in_data2(status_service, year = year_val[1], month = month_val[1]).select(\"MSISDN\", \"SERVICE_CODE\")\ny_next = read_in_data2(status_service, year = year_val[0], month = month_val[0]).select(\"MSISDN\", \"SERVICE_CODE\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "y_prev.createOrReplaceTempView(\"y_prev_view\")\ny_next.createOrReplaceTempView(\"y_next_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "y_prev = spark.sql(\"\"\"SELECT MSISDN, SERVICE_CODE FROM y_prev_view\n                WHERE (SERVICE_CODE=\"BDLEthnicDataH\") OR (SERVICE_CODE=\"BDLDataTazaINT\") \n                \n                OR (SERVICE_CODE=\"BDLIntegLPak\") OR (SERVICE_CODE=\"BDLIntegPak\") \n                OR (SERVICE_CODE=\"BDLIntegLInd\") OR (SERVICE_CODE=\"BDLIntegInd\")\n                OR (SERVICE_CODE=\"BDLIntegLBang\") OR (SERVICE_CODE=\"BDLIntegBang\")\n                OR (SERVICE_CODE=\"BDLAlbania\") OR (SERVICE_CODE=\"BDLVFAlbInt\")\n                \n                OR (SERVICE_CODE=\"BDLXNetData\") OR (SERVICE_CODE=\"BDLPreCombo\")\n                \n                OR (SERVICE_CODE=\"BDLTalkText600\") OR (SERVICE_CODE=\"BDLComboMax\") OR (SERVICE_CODE=\"BDLCUComboXL\")                \n                OR (SERVICE_CODE=\"BDLPasoComboH\") OR (SERVICE_CODE=\"BDLPasoComboXL\") OR (SERVICE_CODE=\"BDLPasoComboML\")\n                OR (SERVICE_CODE=\"BDLPasoComboTL\")\n                \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "y_next = spark.sql(\"\"\"SELECT MSISDN, SERVICE_CODE FROM y_next_view\n                WHERE (SERVICE_CODE=\"BDLEthnicDataH\") OR (SERVICE_CODE=\"BDLDataTazaINT\") \n                \n                OR (SERVICE_CODE=\"BDLIntegLPak\") OR (SERVICE_CODE=\"BDLIntegPak\") \n                OR (SERVICE_CODE=\"BDLIntegLInd\") OR (SERVICE_CODE=\"BDLIntegInd\")\n                OR (SERVICE_CODE=\"BDLIntegLBang\") OR (SERVICE_CODE=\"BDLIntegBang\")\n                OR (SERVICE_CODE=\"BDLAlbania\") OR (SERVICE_CODE=\"BDLVFAlbInt\")\n                \n                OR (SERVICE_CODE=\"BDLXNetData\") OR (SERVICE_CODE=\"BDLPreCombo\")\n                \n                OR (SERVICE_CODE=\"BDLTalkText600\") OR (SERVICE_CODE=\"BDLComboMax\") OR (SERVICE_CODE=\"BDLCUComboXL\")                \n                OR (SERVICE_CODE=\"BDLPasoComboH\") OR (SERVICE_CODE=\"BDLPasoComboXL\") OR (SERVICE_CODE=\"BDLPasoComboML\")\n                OR (SERVICE_CODE=\"BDLPasoComboTL\")\n                \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "                # CU\ny_prev = y_prev.withColumn(\"PRICE\", when(col(\"SERVICE_CODE\") == \"BDLTalkText600\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLComboMax\", 13.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLCUComboXL\", 15)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboH\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboXL\", 10)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboML\", 12)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboTL\", 17.5)\n                # TAZA\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLPak\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLInd\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLBang\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLAlbania\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegPak\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegInd\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegBang\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLVFAlbInt\", 8.5)\n                # INTERNATIONAL\n                .when(col(\"SERVICE_CODE\") == \"BDLEthnicDataH\", 8.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLDataTazaINT\", 10.5)\n                # VFPP\n                .when(col(\"SERVICE_CODE\") == \"BDLXNetData\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLPreCombo\", 13.5)\n                )"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "                # CU\ny_next = y_next.withColumn(\"PRICE\", when(col(\"SERVICE_CODE\") == \"BDLTalkText600\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLComboMax\", 13.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLCUComboXL\", 15)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboH\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboXL\", 10)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboML\", 12)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboTL\", 17.5)\n                # TAZA\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLPak\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLInd\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLBang\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLAlbania\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegPak\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegInd\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegBang\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLVFAlbInt\", 8.5)\n                # INTERNATIONAL\n                .when(col(\"SERVICE_CODE\") == \"BDLEthnicDataH\", 8.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLDataTazaINT\", 10.5)\n                # VFPP\n                .when(col(\"SERVICE_CODE\") == \"BDLXNetData\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLPreCombo\", 13.5)\n                )"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep the highest value bundle\ny_prev = y_prev.select(\"MSISDN\", \"SERVICE_CODE\", \"PRICE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"PRICE\"))).alias(\"row_num\"))\ny_prev = y_prev.filter(col(\"row_num\") == 1).drop(\"row_num\")\n\n### drop duplicates and keep the highest value bundle\ny_next = y_next.select(\"MSISDN\", \"SERVICE_CODE\", \"PRICE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"PRICE\"))).alias(\"row_num\"))\ny_next = y_next.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Rename columns for next month\ny_next = y_next.withColumnRenamed(\"PRICE\", \"PRICE_NEW\")\ny_next = y_next.withColumnRenamed(\"SERVICE_CODE\", \"SERVICE_CODE_NEW\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "y_prev.createOrReplaceTempView(\"y_prev_view\")\ny_next.createOrReplaceTempView(\"y_next_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "migrations = spark.sql(\"\"\"SELECT A.MSISDN, A.PRICE, B.PRICE_NEW\n                                FROM y_prev_view A\n                                LEFT JOIN y_next_view B\n                                ON A.MSISDN= B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "migrations = migrations.na.fill(value= 0, subset=[\"PRICE_NEW\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ADD NEW COLUMN FLAG FOR THOSE WHO PURCHASED HIGHER VALUE BUNDLE THE NEXT MONTH\nmigrations = migrations.withColumn(\"HIGHER_BUNDLE\", when(col(\"PRICE\") < col(\"PRICE_NEW\"), 1).otherwise(0))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#migrations.groupBy(\"HIGHER_BUNDLE\").count().show()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "higher_bundle_segmentation_2 = migrations.select(\"MSISDN\", \"HIGHER_BUNDLE\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Test Base"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "10"}, "execution_count": 187, "metadata": {}, "output_type": "execute_result"}], "source": "month_val[1]"}, {"cell_type": "markdown", "metadata": {}, "source": "### gdpr2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "day_max = 1\ngdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(gdpr, year = year_val[1], month = month_val[1]).select('MSISDN_CLI', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "gdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/day={}/'\n\ndf_gdpr = read_in_data2(gdpr, year = year_val[1], month = month_val[1], day=day_max)"}, {"cell_type": "markdown", "metadata": {}, "source": "### status2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# find max day\nday_max = 1\nstatus = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/'\n\n#print(year_val[1], month_val[1])\nlocals()[\"find_day\"] = read_in_data2(status, year = year_val[1], month = month_val[1]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/day={}/'\n\ndf_status = read_in_data2(status, year = year_val[1], month = month_val[1], day= day_max).select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_status = df_status.select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_status = df_status.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_gdpr.createOrReplaceTempView(\"gdpr_view\")\ndf_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# inner join status with gdpr\nconsent_prepay_status = spark.sql(\"\"\"SELECT A.*\n                         FROM (\n                             SELECT substring(MSISDN, 3 , 10) AS MSISDN, TARIFF_PLAN, CONNECTION_DAY, SMARTPHONE_FLAG, INSERTED\n                             FROM status_view A\n                             WHERE STATUS IN ('A','B')\n                             ) AS A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print((consent_prepay_status.count(), len(consent_prepay_status.columns)))"}, {"cell_type": "markdown", "metadata": {}, "source": "### usage2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# print(month_val[1], year_val[1])\n# print(month_val[2], year_val[2])\n# print(month_val[3], year_val[3])\n# print(month_val[4], year_val[4])\n# print(month_val[5], year_val[5])\n# print(month_val[6], year_val[6])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "[10, 9, 8, 7, 6, 5]"}, "execution_count": 199, "metadata": {}, "output_type": "execute_result"}], "source": "month_val[1:-1]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# find max day available in dataset for each month\nk = 0\nday_max = []\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[1:-1],month_val[1:-1]):\n    k=k+1\n    locals()[\"find_day\"+str(k)] = read_in_data2(usage, year = i, month = j).select('MSISDN', 'day')\n    locals()[\"day_max\"+str(k)] = locals()[\"find_day\"+str(k)].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\n    day_max.append(locals()[\"day_max\"+str(k)])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Read datasets from the previous 6-month period\nk = 0 \nday = 1\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/day={}/'\n\nfor i,j,m in zip(year_val[1:-1], month_val[1:-1], day_max):\n    k=k+1\n    locals()[\"usage_m\"+str(k)] = read_in_data2(usage, year = i, month = j, day = m)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for name in usage_m1.schema.names: usage_m1 = usage_m1.withColumnRenamed(name, name.replace('M1', 'M1'))\nfor name in usage_m2.schema.names: usage_m2 = usage_m2.withColumnRenamed(name, name.replace('M1', 'M2'))\nfor name in usage_m3.schema.names: usage_m3 = usage_m3.withColumnRenamed(name, name.replace('M1', 'M3'))\nfor name in usage_m4.schema.names: usage_m4 = usage_m4.withColumnRenamed(name, name.replace('M1', 'M4'))\nfor name in usage_m5.schema.names: usage_m5 = usage_m5.withColumnRenamed(name, name.replace('M1', 'M5'))\nfor name in usage_m6.schema.names: usage_m6 = usage_m6.withColumnRenamed(name, name.replace('M1', 'M6'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_m2 = usage_m2.withColumnRenamed(\"MSISDN\", \"M2_MSISDN\")\nusage_m3 = usage_m3.withColumnRenamed(\"MSISDN\", \"M3_MSISDN\")\nusage_m4 = usage_m4.withColumnRenamed(\"MSISDN\", \"M4_MSISDN\")\nusage_m5 = usage_m5.withColumnRenamed(\"MSISDN\", \"M5_MSISDN\")\nusage_m6 = usage_m6.withColumnRenamed(\"MSISDN\", \"M6_MSISDN\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_m1.createOrReplaceTempView(\"usage_view_m1\")\nusage_m2.createOrReplaceTempView(\"usage_view_m2\")\nusage_m3.createOrReplaceTempView(\"usage_view_m3\")\nusage_m4.createOrReplaceTempView(\"usage_view_m4\")\nusage_m5.createOrReplaceTempView(\"usage_view_m5\")\nusage_m6.createOrReplaceTempView(\"usage_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Take active base for the last three months\nusage_prepay = spark.sql(\"\"\"SELECT substring(A.MSISDN, 3 , 10) AS MSISDN,\n                     A.M1_TTL_OUT_CALLS, A.M1_TTL_OUT_MINUTES,\n                     A.M1_TTL_OUT_REVENUE, A.M1_V_CALLS_TO_FIXED_LINES, \n                     A.M1_V_MINUTES_TO_FIXED_LINES, A.M1_V_REVENUE_TO_FIXED_LINES, \n                     A.M1_V_CALLS_TO_COMPETITION, A.M1_V_MINUTES_TO_COMPETITION, A.M1_V_REVENUE_TO_COMPETITION,\n                     A.M1_V_CALLS_TO_INTERNATIONAL, A.M1_V_MINUTES_TO_INTERNATIONAL, \n                     A.M1_V_REVENUE_TO_INTERNATIONAL, A.M1_GPRS_SESSION,\n                     A.M1_GPRS_VOLUME, A.M1_GPRS_REVENUE, A.M1_RECHARGES_NUMBER, \n                     A.M1_RECHARGES_VALUE, A.M1_OUT_DAYS, A.M1_INC_DAYS, A.M1_BUNDLE_REVENUE,\n                     \n                     B.M2_TTL_OUT_CALLS, B.M2_TTL_OUT_MINUTES, \n                     B.M2_TTL_OUT_REVENUE, B.M2_V_CALLS_TO_FIXED_LINES,\n                     B.M2_V_MINUTES_TO_FIXED_LINES, B.M2_V_REVENUE_TO_FIXED_LINES, \n                     B.M2_V_CALLS_TO_COMPETITION, B.M2_V_MINUTES_TO_COMPETITION, \n                     B.M2_V_REVENUE_TO_COMPETITION, B.M2_V_CALLS_TO_INTERNATIONAL,\n                     B.M2_V_MINUTES_TO_INTERNATIONAL, B.M2_V_REVENUE_TO_INTERNATIONAL, \n                     B.M2_GPRS_SESSION, B.M2_GPRS_VOLUME, \n                     B.M2_GPRS_REVENUE, B.M2_RECHARGES_NUMBER, \n                     B.M2_RECHARGES_VALUE, B.M2_OUT_DAYS,\n                     B.M2_INC_DAYS, B.M2_BUNDLE_REVENUE,\n                     \n                     C.M3_TTL_OUT_CALLS, C.M3_TTL_OUT_MINUTES, \n                     C.M3_TTL_OUT_REVENUE, C.M3_V_CALLS_TO_FIXED_LINES,\n                     C.M3_V_MINUTES_TO_FIXED_LINES, C.M3_V_REVENUE_TO_FIXED_LINES, \n                     C.M3_V_CALLS_TO_COMPETITION, C.M3_V_MINUTES_TO_COMPETITION, \n                     C.M3_V_REVENUE_TO_COMPETITION, C.M3_V_CALLS_TO_INTERNATIONAL,\n                     C.M3_V_MINUTES_TO_INTERNATIONAL, C.M3_V_REVENUE_TO_INTERNATIONAL, \n                     C.M3_GPRS_SESSION, C.M3_GPRS_VOLUME, \n                     C.M3_GPRS_REVENUE, C.M3_RECHARGES_NUMBER, \n                     C.M3_RECHARGES_VALUE, C.M3_OUT_DAYS,\n                     C.M3_INC_DAYS, C.M3_BUNDLE_REVENUE,\n                     \n                     D.M4_TTL_OUT_CALLS, D.M4_TTL_OUT_MINUTES, \n                     D.M4_TTL_OUT_REVENUE, D.M4_V_CALLS_TO_FIXED_LINES,\n                     D.M4_V_MINUTES_TO_FIXED_LINES, D.M4_V_REVENUE_TO_FIXED_LINES, \n                     D.M4_V_CALLS_TO_COMPETITION, D.M4_V_MINUTES_TO_COMPETITION, \n                     D.M4_V_REVENUE_TO_COMPETITION, D.M4_V_CALLS_TO_INTERNATIONAL,\n                     D.M4_V_MINUTES_TO_INTERNATIONAL, D.M4_V_REVENUE_TO_INTERNATIONAL, \n                     D.M4_GPRS_SESSION, D.M4_GPRS_VOLUME, \n                     D.M4_GPRS_REVENUE, D.M4_RECHARGES_NUMBER, \n                     D.M4_RECHARGES_VALUE, D.M4_OUT_DAYS, \n                     D.M4_INC_DAYS, D.M4_BUNDLE_REVENUE,\n                     \n                     E.M5_TTL_OUT_CALLS, E.M5_TTL_OUT_MINUTES, \n                     E.M5_TTL_OUT_REVENUE, E.M5_V_CALLS_TO_FIXED_LINES,\n                     E.M5_V_MINUTES_TO_FIXED_LINES, E.M5_V_REVENUE_TO_FIXED_LINES, \n                     E.M5_V_CALLS_TO_COMPETITION, E.M5_V_MINUTES_TO_COMPETITION, \n                     E.M5_V_REVENUE_TO_COMPETITION, E.M5_V_CALLS_TO_INTERNATIONAL,\n                     E.M5_V_MINUTES_TO_INTERNATIONAL, E.M5_V_REVENUE_TO_INTERNATIONAL, \n                     E.M5_GPRS_SESSION, E.M5_GPRS_VOLUME, \n                     E.M5_GPRS_REVENUE, E.M5_RECHARGES_NUMBER, \n                     E.M5_RECHARGES_VALUE, E.M5_OUT_DAYS,\n                     E.M5_INC_DAYS, E.M5_BUNDLE_REVENUE,\n                  \n                     F.M6_TTL_OUT_CALLS, F.M6_TTL_OUT_MINUTES, \n                     F.M6_TTL_OUT_REVENUE, F.M6_V_CALLS_TO_FIXED_LINES,\n                     F.M6_V_MINUTES_TO_FIXED_LINES, F.M6_V_REVENUE_TO_FIXED_LINES, \n                     F.M6_V_CALLS_TO_COMPETITION, F.M6_V_MINUTES_TO_COMPETITION, \n                     F.M6_V_REVENUE_TO_COMPETITION, F.M6_V_CALLS_TO_INTERNATIONAL,\n                     F.M6_V_MINUTES_TO_INTERNATIONAL, F.M6_V_REVENUE_TO_INTERNATIONAL, \n                     F.M6_GPRS_SESSION, F.M6_GPRS_VOLUME, \n                     F.M6_GPRS_REVENUE, F.M6_RECHARGES_NUMBER, \n                     F.M6_RECHARGES_VALUE, F.M6_OUT_DAYS,\n                     F.M6_INC_DAYS, F.M6_BUNDLE_REVENUE\n                     \n                     from usage_view_m1 A\n                     left join usage_view_m2 B\n                          on A.MSISDN = B.M2_MSISDN \n                     left join usage_view_m3 C\n                          on A.MSISDN = C.M3_MSISDN\n                     left join usage_view_m4 D\n                          on A.MSISDN = D.M4_MSISDN\n                     left join usage_view_m5 E\n                          on A.MSISDN = E.M5_MSISDN\n                     left join usage_view_m6 F\n                          on A.MSISDN = F.M6_MSISDN\n                          \n                     WHERE ((A.M1_TTL_OUT_CALLS > 0) or (A.M1_GPRS_SESSION > 0))\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_prepay = usage_prepay.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_prepay.createOrReplaceTempView(\"usage_prepay_view\")\ndf_gdpr.createOrReplaceTempView(\"gdpr_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# join usage with consent\nconsent_prepay_usage = spark.sql(\"\"\"SELECT A.*\n                         FROM usage_prepay_view A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\") "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print((consent_prepay_usage.count(), len(consent_prepay_usage.columns)))"}, {"cell_type": "markdown", "metadata": {}, "source": "# join status with usage"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "consent_prepay_usage.createOrReplaceTempView(\"consent_prepay_usage_view\")\nconsent_prepay_status.createOrReplaceTempView(\"consent_prepay_status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "base_usage_status = spark.sql(\"\"\"SELECT A.*, B.TARIFF_PLAN, B.CONNECTION_DAY, B.SMARTPHONE_FLAG, B.INSERTED\n                             FROM consent_prepay_usage_view A\n                             INNER JOIN consent_prepay_status_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])\n    elif (dict(base_usage_status.dtypes)[c] == 'object' or dict(base_usage_status.dtypes)[c] == 'string' or dict(base_usage_status.dtypes)[c] == 'timestamp'):\n        base_usage_status = base_usage_status.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print((base_usage_status.count(), len(base_usage_status.columns)))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Add/Convert_Features2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 5. Convert from second -> minutes\n# 6. Convert from KByte -> MByte\nfor column in base_usage_status.columns:\n    if 'MINUTES' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/60)\n    if 'VOLUME' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/1024)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Group tariffs"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# CU\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba40', 'CU'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba', 'CU'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VFPP\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'VALCBASE', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'HAM', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Advanced', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'OCFP', 'VFPP'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ETHNIC (INTERNATIONAL + TAZA)\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'INTPACK', 'INTERNATIONAL'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Taza', 'TAZA'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'TAZA', 'TAZA'))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Tenure"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Calculate tenure in months\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", round(F.months_between(col(\"INSERTED\"), col(\"CONNECTION_DAY\"))))\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", col(\"TENURE_IN_MONTHS\").cast(IntegerType()))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# SELECT ONLY CUSTOMERS THAT ARE MORE THAN 3 MONTHS IN OUR DATABASE\nbase_usage_status = base_usage_status[base_usage_status[\"TENURE_IN_MONTHS\"] > 3] "}, {"cell_type": "markdown", "metadata": {}, "source": "# Average talk per time"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1,7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+ str(month) + \"_MINUTES_PER_CALL\", \n                                                     col(\"M\" + str(month) + \"_TTL_OUT_MINUTES\") / col(\"M\" + str(month) + \"_TTL_OUT_CALLS\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# ARPU"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+\"_ARPU\", \n                        col(\"M\"+str(month)+\"_TTL_OUT_REVENUE\") + col(\"M\"+str(month)+\"_GPRS_REVENUE\") \n                                                     + col(\"M\"+str(month)+\"_BUNDLE_REVENUE\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# ROC"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        prev_month_avg = (col(\"M2\"+column[2:]) + col(\"M3\"+column[2:]) + col(\"M4\"+column[2:]) + col(\"M5\"+column[2:]) + col(\"M6\"+column[2:])) / 5 \n        base_usage_status = base_usage_status.withColumn(\"M1_ROC\"+column[2:], (col(\"M1\"+column[2:]) -  prev_month_avg) / prev_month_avg )"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Average for half and all semester"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\nfor column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        # first semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M13_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:]))/3)\n        # second semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M46_AVG\"+ column[2:], (col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:]))/3)\n        # calculate the average for all six months     \n        base_usage_status = base_usage_status.withColumn(\"M16_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:])+col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:])) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Ratio racharge/bundle value"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+ \"_EXPENDITURE_RATIO\",\n                                                     col(\"M\"+str(month)+ \"_RECHARGES_VALUE\") / col(\"M\"+str(month)+ \"_BUNDLE_REVENUE\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### demographics2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# CAR LINE DATASET\nline = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(line, year = year_val[1], month = month_val[1]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "line = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/day={}/'\n\ndf_line = read_in_data2(line, year = year_val[1], month = month_val[1], day= day_max)\ndf_line = df_line.select(\"MSISDN\", \"RETAIL_CUST_ACCT_DWH_ID\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\",\"RETAIL_CUST_ACCT_DWH_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "base_usage_status.createOrReplaceTempView(\"base_usage_view\")\ndf_line.createOrReplaceTempView(\"line_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Join CAR_LINE with BASE_USAGE\nusage_status_df = spark.sql (\"\"\"SELECT A.*, B.RETAIL_CUST_ACCT_DWH_ID\n                          FROM base_usage_view A\n                          INNER JOIN line_view B\n                             ON A.MSISDN = B.MSISDN\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# DEMOGRAPHICS DATASET\ndemographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(demographics, year = year_val[1], month = month_val[1]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "demographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/day={}/'\n\ndf_demographics = read_in_data2(demographics, year = year_val[1], month = month_val[1], day= day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# select specific columns\ndf_demographics = df_demographics.select(\"CUST_DWH_ID\", \"POST_CODE\", \"GENDER\", \"AGE\", \"VF_COMBO_FLG\", \"ACTIVE_TOTAL_LINES\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.dropDuplicates([\"CUST_DWH_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn('POST_CODE', regexp_replace('POST_CODE', 'XXXXX', 'DUMMY'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if (dict(df_demographics.dtypes)[column] == 'int64' or dict(df_demographics.dtypes)[column] == 'double' or\n       dict(df_demographics.dtypes)[column] == 'int'):\n        # fill with mean\n        mean = df_demographics.agg({column: \"avg\"}).collect()[0][0]\n        df_demographics = df_demographics.na.fill(mean, subset=[column])\n    elif (dict(df_demographics.dtypes)[column] == 'object' or dict(df_demographics.dtypes)[column] == 'string'):\n        if (column == \"GENDER\"):\n            df_demographics = df_demographics.na.fill(value=\"O\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n        if (column == \"POST_CODE\"):\n            df_demographics = df_demographics.na.fill(value=\"DUMMY\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n        df_demographics = df_demographics.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# STRANGE VALUES FOR AGES\ndf_demographics = df_demographics.withColumn(\"AGE\", coalesce(col(\"AGE\"), lit(0.0)))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") > 80, 80).otherwise(col(\"AGE\")))\ndf_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") < 17, 18).otherwise(col(\"AGE\")))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join usage-status wth demographics"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics.createOrReplaceTempView(\"demographics_view\")\nusage_status_df.createOrReplaceTempView(\"usage_status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# JOIN\nusage_status_demo = spark.sql (\"\"\"SELECT A.*, B.POST_CODE, B.GENDER, B.AGE, B.VF_COMBO_FLG, B.ACTIVE_TOTAL_LINES\n                          FROM usage_status_view A\n                          LEFT JOIN demographics_view B\n                             ON A.RETAIL_CUST_ACCT_DWH_ID = B.CUST_DWH_ID\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo = usage_status_demo.drop(col(\"RETAIL_CUST_ACCT_DWH_ID\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if column != 'CUST_DWH_ID':\n        if (dict(usage_status_demo.dtypes)[column] == 'int64' or dict(usage_status_demo.dtypes)[column] == 'double' or\n       dict(usage_status_demo.dtypes)[column] == 'int'):\n            # fill with mean\n            mean = usage_status_demo.agg({column: \"avg\"}).collect()[0][0]\n            usage_status_demo = usage_status_demo.na.fill(mean, subset=[column])\n        elif (dict(usage_status_demo.dtypes)[column] == 'object' or dict(usage_status_demo.dtypes)[column] == 'string'):\n            if (column == \"GENDER\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"O\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n            if (column == \"POST_CODE\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"DUMMY\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n            usage_status_demo = usage_status_demo.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### post_code2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "0"}, "execution_count": 246, "metadata": {}, "output_type": "execute_result"}], "source": "subprocess.call('/bin/sh /usr/bin/gsutil -q cp gs://' + files_bucket + '/notebooks/jupyter/higher_bundles/Sociodemographics.xlsx Sociodemographics.xlsx', shell=True)\nPopulation_pools = pd.read_excel('Sociodemographics.xlsx')\npopulation_pools_df = sql.createDataFrame(Population_pools)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.drop(\"Postcode_key\", \"Name\", \"Periferiaki_enotita\", \"Population_aged_60+\",\n                                              \"Male_Population_aged_60+\", \"Female_Population_aged_60+\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.withColumnRenamed('Population_aged_0-14', 'Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_0-14', 'Male_Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_0-14', 'Female_Population_aged_0_14')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_15-29', 'Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_15-29', 'Male_Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_15-29', 'Female_Population_aged_15_29')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_30-44', 'Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_30-44', 'Male_Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_30-44', 'Female_Population_aged_30_44')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_45-59', 'Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_45-59', 'Male_Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_45-59', 'Female_Population_aged_45_59')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_million_Euro', 'Purchasing_Power_million_Euro')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_per_mill_of_country', 'Purchasing_Power_per_mill_of_country')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_Euro_per_capita', 'Purchasing_Power_Euro_per_capita')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_index_(country_eq.100)', 'Purchasing_Power_index_country_eq_100')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df.createOrReplaceTempView(\"population_pools_view\")\nusage_status_demo.createOrReplaceTempView(\"usage_status_demo_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Exclude ages from 60+\nusage_status_demo_pc = spark.sql (\"\"\"SELECT A.*, B.Population, B.Households, B.Average_Household_Size, B.Male_Population,\n                          B.Female_Population, B.Population_aged_0_14, B.Male_Population_aged_0_14, B.Female_Population_aged_0_14,\n                          B.Population_aged_15_29, B.Male_Population_aged_15_29, B.Female_Population_aged_15_29, \n                          B.Population_aged_30_44, B.Male_Population_aged_30_44, B.Female_Population_aged_30_44,\n                          B.Population_aged_45_59, B.Male_Population_aged_45_59, B.Female_Population_aged_45_59,\n                          B.Purchasing_Power_million_Euro, B.Purchasing_Power_per_mill_of_country,\n                          B.Purchasing_Power_Euro_per_capita, B.Purchasing_Power_index_country_eq_100\n\n                          FROM usage_status_demo_view A                          \n                          LEFT JOIN population_pools_view B                          \n                          ON A.POST_CODE = B.POST_CODE\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in population_pools_df.columns:\n    usage_status_demo_pc = usage_status_demo_pc.na.fill(value=0, subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### students2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(status_service, year = year_val[1], month = month_val[1]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/day={}'\n\ndf_status_service = read_in_data2(status_service, year = year_val[1], month = month_val[1], day=day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_status_service.createOrReplaceTempView(\"df_status_service_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_students = spark.sql(\"\"\"SELECT * \n                           FROM df_status_service_view A\n                           WHERE SERVICE_CODE == 'BDLCUPaso' \n                           \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_students = df_students.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ADD A NEW COLUMN WITH A FLAG TO INIDICATE THAT THIS USER IS STUDENT\ndf_students = df_students.withColumn(\"STUDENTS_FLAG\", lit(\"Y\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# LEFT JOIN USAGE_STATUS_DEMO WITH STUDENTS INFO"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc.createOrReplaceTempView(\"usage_status_demo_pc_view\")\ndf_students.createOrReplaceTempView(\"students_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud = spark.sql(\"\"\"SELECT A.*, B.STUDENTS_FLAG\n                                FROM usage_status_demo_pc_view A\n                                LEFT JOIN students_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# SET indicator N for NO if the user is not a students\nusage_status_demo_pc_stud = usage_status_demo_pc_stud.na.fill(value=\"N\", subset=[\"STUDENTS_FLAG\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### buckets2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "[10, 9]"}, "execution_count": 261, "metadata": {}, "output_type": "execute_result"}], "source": "month_val[1:3]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 2-month period\nk = 0 \nday = 1\nbuckets = 'gs://'+ model_outputs_bucket + '/prepay_buckets/result/parquet/1.0/year={}/month={}/'\n\nfor i,j in zip(year_val[1:3], month_val[1:3]):\n    k=k+1\n    locals()[\"buckets_m\"+str(k)] = read_in_data2(buckets, year = i, month = j)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets = buckets_m1.union(buckets_m2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets = df_buckets.sort(col(\"BUNDLE\").asc(), col(\"ACTIVATION_DATE\").desc())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep first occurrence for each bundle\ndf_buckets = df_buckets.select(\"MSISDN\", \"ACTIVATION_DATE\", \"BUNDLE\", \"BUNDLE_REVENUE\", \"BALANCE\", \"VOICE_BUCKET\", \"DATA_BUCKET\", \"SMS_BUCKET\",\n                                   F.row_number().over(Window.partitionBy(\"MSISDN\", \"BUNDLE\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_buckets = df_buckets.filter(col(\"row_num\") == 1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Calculate summary for all buckets\ndf_buckets_summary = df_buckets.groupBy(\"MSISDN\").agg(sum(\"VOICE_BUCKET\").alias(\"VOICE_BUCKET_SUMMARY\"),sum(\"DATA_BUCKET\").alias(\"DATA_BUCKET_SUMMARY\"),sum(\"SMS_BUCKET\").alias(\"SMS_BUCKET_SUMMARY\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "### balance2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep last registration \ndf_balance = df_buckets.select(\"MSISDN\", \"BALANCE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_balance_summary = df_balance.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join bucket with balance"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets_summary.createOrReplaceTempView(\"buckets_sum_view\")\ndf_balance_summary.createOrReplaceTempView(\"balance_sum_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets_balance = spark.sql(\"\"\"SELECT A.*, B.BALANCE\n                             FROM buckets_sum_view A\n                             INNER JOIN balance_sum_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join bucket balance with main dataset "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud.createOrReplaceTempView(\"usage_status_demo_pc_stud_view\")\ndf_buckets_balance.createOrReplaceTempView(\"buckets_balance_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets = spark.sql(\"\"\"SELECT A.*, B.VOICE_BUCKET_SUMMARY, B.DATA_BUCKET_SUMMARY, B.SMS_BUCKET_SUMMARY, B.BALANCE\n                                FROM usage_status_demo_pc_stud_view A\n                                LEFT JOIN buckets_balance_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for c in df_buckets_balance.columns:\n    if (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'int64' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'double' or\n       dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'int'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=0, subset=[c])\n    elif (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'object' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'string' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'timestamp'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### drop_calls2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "drop_calls = 'gs://'+ mediatedcdrs_bucket +'/eds_network_cdr/2.0/parquet/year={}/month={}/'\n\ndf_drop_calls = read_in_data2(drop_calls, year = year_val[1], month = month_val[1])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_drop_calls = df_drop_calls.select(\"SAMPLED\",\"A_NUMBER\",\"FIRST_LAC\",\"LAST_LAC\",\"CELL\",\"LAST_CELL\",\"REC_TYPE\",\"TARIFF\",\n                               \"DURATION\", \"TERM_CAUSE\", \"day\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_drop_calls.createOrReplaceTempView(\"cdrs_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "drop_calls_query = spark.sql(\"\"\"SELECT K.MSISDN,\n                                count(*) as DROPPED_CALL_COUNT\n                                FROM (\n                                    SELECT L.MSISDN, L.SAMPLED, L.YEAR\n                                    FROM (\n                                       SELECT\n                                       A.SAMPLED, A.A_NUMBER AS MSISDN, A.day, YEAR(A.SAMPLED) AS YEAR, SUBSTR(A.TERM_CAUSE,1,4) AS EOS,\n                                       CASE WHEN A.FIRST_LAC LIKE '%IE%' THEN A.LAST_LAC  --this condition holds only for calls\n                                           WHEN A.FIRST_LAC = '' THEN A.LAST_LAC          --this condition holds only for calls\n                                           ELSE A.FIRST_LAC END FIRST_LAC,                --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_LAC = '' THEN A.FIRST_LAC         --this condition holds for both calls and SMS\n                                           ELSE A.LAST_LAC END LAST_LAC,                  --this condition holds for both calls and SMS\n                                       CASE WHEN A.CELL LIKE '%F%' THEN A.LAST_CELL       --this condition holds only for calls\n                                           WHEN A.CELL = '' THEN A.LAST_CELL              --this condition holds only for calls\n                                           ELSE A.CELL END CELL,                          --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_CELL = '' THEN A.CELL             --this condition holds for both calls and SMS\n                                           ELSE A.LAST_CELL END LAST_CELL,                --this condition holds for both calls and SMS\n                                       ROW_NUMBER() OVER (PARTITION BY A.A_NUMBER, A.SAMPLED ORDER BY A.A_NUMBER, A.SAMPLED) as LEVEL\n                                       FROM cdrs_view A\n                                       WHERE ((A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL != '') OR  -- this condition holds only for calls\n                                       (A.FIRST_LAC = '' AND A.LAST_LAC != '' AND A.CELL = '' AND A.LAST_CELL != '') OR    -- this condition holds only for calls\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC = '' AND A.CELL != '' AND A.LAST_CELL = '') OR    -- this condition holds for both calls and SMS\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL = ''))     -- this condition holds for both calls and SMS\n                                       AND A.REC_TYPE IN ('20','30') AND A.TARIFF != '142'\n                                       AND LENGTH(A.TERM_CAUSE) = 8\n                                       AND SUBSTR(A.TERM_CAUSE,1,4) IN ('068F','08BF','09A6','09C3','09C5','09C8','09F8','0A0E','0A0F','0AE9','0C15','0CD2',\n                                                                        '0CD3','0F7B','0F7C','018F','065D','065E','0700','0701','0702','09A7','09BF','09C0',\n                                                                        '09C2','09C4','09C6','09C7','09C9','09F6','09F7','0A0A','0A0B','0A0C','0A0D','0C14',\n                                                                        '0C16','0F7D','1C8F','1C90','1C91','1C92','1C9A','1C9B')\n                                       AND A.A_NUMBER != '' AND A.A_NUMBER LIKE '69%' \n                                       --ORDER BY A.A_NUMBER, A.SAMPLED\n                                       ) AS L\n                                    WHERE L.LEVEL = 1\n                                    --ORDER BY L.MSISDN, L.SAMPLED ASC\n                                ) K\n                                GROUP BY K.MSISDN\n                                ORDER BY K.MSISDN\n                             \"\"\")  "}, {"cell_type": "markdown", "metadata": {}, "source": "# Join drop_calls with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_view\")\ndrop_calls_query.createOrReplaceTempView(\"drop_calls_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = spark.sql(\"\"\"SELECT A.*, B.DROPPED_CALL_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_view A\n                                LEFT JOIN drop_calls_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = usage_status_demo_pc_stud_buckets_dropcalls.na.fill(value=0, subset=[\"DROPPED_CALL_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### tickets2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ticket_service = 'gs://'+ dhdwh_bucket +'/mobile_sr_tt/1.0/parquet/year={}/month={}/'\n\ndf_tickets_requests = read_in_data2(ticket_service, year = year_val[1], month = month_val[1]).select(\"X_MSISDN\",\"SR_ID\").drop('service_file_id')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_tickets_requests = df_tickets_requests.select(\"X_MSISDN\",\"SR_ID\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# drop duplicates\ndf_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\", \"SR_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ADD A COLUMN AS TICKETS COUNTER FOR EACH MSISDN\ndf_tickets_requests = df_tickets_requests.select(\"X_MSISDN\", F.count(\"X_MSISDN\").over(Window.partitionBy(\"X_MSISDN\")).alias(\"TICKETS_COUNT\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join tickets with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_view\")\ndf_tickets_requests.createOrReplaceTempView(\"tickets_requests_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = spark.sql(\"\"\"SELECT A.*, B.TICKETS_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_view A\n                                LEFT JOIN tickets_requests_view B\n                                ON A.MSISDN=B.X_MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = usage_status_demo_pc_stud_buckets_dropcalls_tickets.na.fill(value=0, subset=[\"TICKETS_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### channel2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "events = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'\n\ndf_events = read_in_data2(events, year = year_val[1], month = month_val[1])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_events.createOrReplaceTempView(\"df_events_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = spark.sql(\"\"\"SELECT ACCOUNT_ID, REQUESTING_SYSTEM                             \n                             FROM df_events_view \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'VOP', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUapp', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'MCare', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUsite', 'DIGITAL'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'TAZAAPP', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'EKIOSK', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PostpaidToPrepaid', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'XPCVM', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'LMG', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PEGA', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'SMSVAS', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CRM', 'OTHER'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# MIA KATHGORIA VFSHOP - (VFSHOP)\n# ALLH KATHGORIA - THLEFWNO (IVR)  \n# DIGITAL (VOP, MCARE, CUapp, CUsite)\n# OTHER"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# One-hot encoding - 4 kathgories\ndf_bundle_purchase = df_bundle_purchase.groupBy('ACCOUNT_ID').pivot('REQUESTING_SYSTEM').count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for c in df_bundle_purchase.columns:\n        df_bundle_purchase = df_bundle_purchase.na.fill(0, subset=[c])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = df_bundle_purchase.select(\"ACCOUNT_ID\", \"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join channel with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_view\")\ndf_bundle_purchase.createOrReplaceTempView(\"bundle_purchase_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = spark.sql(\"\"\"SELECT A.*, B.DIGITAL, B.VFShop, B.IVR, B.OTHER\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_view A\n                                LEFT JOIN bundle_purchase_view B\n                                ON A.MSISDN= SUBSTR(B.ACCOUNT_ID,3,10)\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.na.fill(value=0, subset=[\"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "# NumberOfBundles2"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 6-month period\nk = 0 \nday = 1\nevents = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[1:-1], month_val[1:-1]):\n    k=k+1\n    locals()[\"events_m\"+str(k)] = read_in_data2(events, year = i, month = j).select(\"ACCOUNT_ID\", \"PTP_COSP_AMA_CODE\", \"EVENT_LABEL\", \"EVENT_RESULT\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "events_m1.createOrReplaceTempView(\"events_view_m1\")\nevents_m2.createOrReplaceTempView(\"events_view_m2\")\nevents_m3.createOrReplaceTempView(\"events_view_m3\")\nevents_m4.createOrReplaceTempView(\"events_view_m4\")\nevents_m5.createOrReplaceTempView(\"events_view_m5\")\nevents_m6.createOrReplaceTempView(\"events_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m1 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m2 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m2 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m3 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m3 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m4 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m4 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m5 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m5 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m6 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m6 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1.createOrReplaceTempView(\"num_of_bundles_view_m1\")\ndf_num_of_bundles_m2.createOrReplaceTempView(\"num_of_bundles_view_m2\")\ndf_num_of_bundles_m3.createOrReplaceTempView(\"num_of_bundles_view_m3\")\ndf_num_of_bundles_m4.createOrReplaceTempView(\"num_of_bundles_view_m4\")\ndf_num_of_bundles_m5.createOrReplaceTempView(\"num_of_bundles_view_m5\")\ndf_num_of_bundles_m6.createOrReplaceTempView(\"num_of_bundles_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = spark.sql(\"\"\"SELECT substring(A.ACCOUNT_ID, 3 , 10) AS ACCOUNT_ID, A.BUNDLES_NUM AS M1_BUNDLES_NUM, \n                     B.BUNDLES_NUM AS M2_BUNDLES_NUM, C.BUNDLES_NUM AS M3_BUNDLES_NUM, \n                     D.BUNDLES_NUM AS M4_BUNDLES_NUM,\n                     E.BUNDLES_NUM AS M5_BUNDLES_NUM, \n                     F.BUNDLES_NUM AS M6_BUNDLES_NUM\n                     \n                     FROM num_of_bundles_view_m1 A                 \n                    \n                     left join num_of_bundles_view_m2 B\n                          on A.ACCOUNT_ID = B.ACCOUNT_ID \n                     left join num_of_bundles_view_m3 C \n                          on A.ACCOUNT_ID = C.ACCOUNT_ID\n                     left join num_of_bundles_view_m4 D \n                          on A.ACCOUNT_ID = D.ACCOUNT_ID\n                     left join num_of_bundles_view_m5 E \n                          on A.ACCOUNT_ID = E.ACCOUNT_ID\n                     left join num_of_bundles_view_m6 F\n                          on A.ACCOUNT_ID = F.ACCOUNT_ID\n                          \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = num_of_bundles_semester.na.fill(value=0)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\n# first semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M13_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\"))/3)\n# second semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M46_AVG_BUNDLES_NUM\", (col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\"))/3)\n# calculate the average for all six months     \nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M16_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\")+col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\")) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join number of bundles with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view\")\nnum_of_bundles_semester.createOrReplaceTempView(\"num_of_bundles_semester_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = spark.sql(\"\"\"SELECT A.*, B.M1_BUNDLES_NUM, B.M2_BUNDLES_NUM, B.M3_BUNDLES_NUM, \n                                B.M4_BUNDLES_NUM, B.M5_BUNDLES_NUM, B.M6_BUNDLES_NUM, B.M13_AVG_BUNDLES_NUM, B.M46_AVG_BUNDLES_NUM, \n                                B.M16_AVG_BUNDLES_NUM\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view A\n                                LEFT JOIN num_of_bundles_semester_view B\n                                ON A.MSISDN= B.ACCOUNT_ID\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.na.fill(value=0)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print((usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.count(), len(usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.columns)))"}, {"cell_type": "markdown", "metadata": {}, "source": "# JOIN ALL DATASET WITH Y"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.createOrReplaceTempView(\"final_dataset_view\")\nhigher_bundle_segmentation_2.createOrReplaceTempView(\"higher_bundle_segmentation_2_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "final_df_test = spark.sql(\"\"\"SELECT A.*, B.HIGHER_BUNDLE\n                                FROM final_dataset_view A\n                                LEFT JOIN higher_bundle_segmentation_2_view B\n                                ON A.MSISDN= B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "final_df_test = final_df_test.na.fill(value=0, subset=[\"HIGHER_BUNDLE\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#final_df_test.groupBy(\"HIGHER_BUNDLE\").count().show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# ------------------------------------------------------------------------------------------------------#"}, {"cell_type": "markdown", "metadata": {}, "source": "### Modelling <a class=\"anchor\" id=\"modelling\"></a>"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Boruta features\nboruta_features = ['MSISDN', 'M1_TTL_OUT_CALLS', 'M1_V_CALLS_TO_COMPETITION', 'M1_V_MINUTES_TO_COMPETITION', \n          'M1_GPRS_SESSION', 'M1_GPRS_VOLUME', 'M1_RECHARGES_VALUE', 'M1_OUT_DAYS', 'M1_INC_DAYS', \n          'M1_BUNDLE_REVENUE', 'M2_RECHARGES_VALUE', 'M2_OUT_DAYS', 'M2_INC_DAYS', 'M2_BUNDLE_REVENUE', \n          'M3_V_CALLS_TO_COMPETITION', 'M3_GPRS_VOLUME', 'M3_BUNDLE_REVENUE', 'M4_GPRS_VOLUME', 'M4_INC_DAYS', \n          'M4_BUNDLE_REVENUE', 'M5_V_MINUTES_TO_COMPETITION', 'M5_BUNDLE_REVENUE', 'M6_V_MINUTES_TO_COMPETITION',\n          'M6_BUNDLE_REVENUE', 'M1_ARPU', 'M2_ARPU', 'M4_ARPU', 'M5_ARPU', 'M1_ROC_TTL_OUT_CALLS',\n          'M13_AVG_TTL_OUT_CALLS', 'M13_AVG_TTL_OUT_MINUTES', 'M13_AVG_V_CALLS_TO_COMPETITION',\n          'M13_AVG_V_MINUTES_TO_COMPETITION', 'M13_AVG_GPRS_VOLUME', 'M16_AVG_GPRS_VOLUME', \n          'M13_AVG_RECHARGES_VALUE', 'M46_AVG_RECHARGES_VALUE', 'M16_AVG_RECHARGES_VALUE', 'M13_AVG_OUT_DAYS',\n          'M16_AVG_OUT_DAYS', 'M13_AVG_INC_DAYS', 'M16_AVG_INC_DAYS', 'M13_AVG_BUNDLE_REVENUE',\n          'M46_AVG_BUNDLE_REVENUE', 'M16_AVG_BUNDLE_REVENUE', 'M1_EXPENDITURE_RATIO', 'M2_EXPENDITURE_RATIO', \n          'STUDENTS_FLAG', 'VOICE_BUCKET_SUMMARY', 'DATA_BUCKET_SUMMARY', 'SMS_BUCKET_SUMMARY', 'BALANCE', \n          'DIGITAL', 'TARIFF_PLAN' ,'M1_TTL_OUT_MINUTES', 'M2_RECHARGES_NUMBER', 'M3_ARPU', 'M1_ROC_V_CALLS_TO_COMPETITION',\n          'M16_AVG_TTL_OUT_CALLS', 'M13_AVG_RECHARGES_NUMBER', 'M46_AVG_INC_DAYS', 'M13_AVG_ARPU', 'M46_AVG_ARPU', 'HIGHER_BUNDLE']"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#final_df_train.select(boruta_features).limit(5).toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Convert to Pandas\ndf_train = final_df_train.select(boruta_features).toPandas()\ndf_test = final_df_test.select(boruta_features).toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "### Preprocessing"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Train set\n#Downsampling\ndf_train_sampled = downsampling(df_train, n_majority= 100000)  # change this number\n# Define X and y\nX_train, y_train = split_x_y(df_train_sampled)\n# Encoding\nX_train, y_train = encoding_data(X_train, y_train)\n# Save feature name\nfeature_names= np.array(X_train.columns)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Test set\n# Define X and y\nX_test, y_test = split_x_y(df_test)\n# Encoding\nX_test, y_test = encoding_data(X_test, y_test)\n# Save feature name\nfeature_names= np.array(X_test.columns)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "previous_proportion = estimate_proportion(y_train)\n#print(previous_proportion)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "X_train_new = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Grid_Search"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#param_test = {'boosting_type': ['gbdt', 'dart', 'goss'], 'n_estimators':[100, 500, 1000], 'learning_rate': [0.01, 0.1, 0.2],\n#             'scale_pos_weight': [2, 3, 4]}"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#import sklearn\n#sklearn.metrics.SCORERS.keys()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Training"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "lgbm = lgb.LGBMClassifier(boosting_type = 'gbdt',\n                                    importance_type = 'gain',\n                                    scale_pos_weight= 3,\n                                    n_estimators = 1000,\n                                    learning_rate=0.01, metric='auc', silent=True, n_jobs=-1, random_state=10)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "LGBMClassifier(importance_type='gain', learning_rate=0.01, metric='auc',\n               n_estimators=1000, random_state=10, scale_pos_weight=3,\n               silent=True)"}, "execution_count": 330, "metadata": {}, "output_type": "execute_result"}], "source": "lgbm.fit(X_train_new, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "[[896623 378945]\n [  7366  20581]]\n0.7036390068392001\n                  precision    recall  f1-score   support\n\nNo higher bundle       0.99      0.70      0.82   1275568\n   Higher Bundle       0.05      0.74      0.10     27947\n\n        accuracy                           0.70   1303515\n       macro avg       0.52      0.72      0.46   1303515\n    weighted avg       0.97      0.70      0.81   1303515\n\n"}], "source": "cm, acc, class_report = evaluation(X_test, y_test, lgbm)\nprint(cm)\nprint(acc)\nprint(class_report)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#lgbm = lgb.LGBMClassifier()\n#grid_lgbm = GridSearchCV(lgbm, param_grid=param_test, cv=3, scoring='recall_macro', n_jobs=-1, verbose=3)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#grid_lgbm.fit(X_train_new, y_train)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print(\" Results from Grid Search \")\n#print(\"\\n The best estimator across ALL searched params:\\n\",grid_lgbm.best_estimator_)\n#print(\"\\n The best score across ALL searched params:\\n\",grid_lgbm.best_score_)\n#print(\"\\n The best parameters across ALL searched params:\\n\",grid_lgbm.best_params_)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#lgbm = grid_lgbm.best_estimator_"}, {"cell_type": "markdown", "metadata": {}, "source": "# Evaluation"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#cm, acc, class_report = evaluation(X_test, y_test, lgbm)\n#print(cm)\n#print(acc)\n#print(class_report)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "array([0.61972353, 0.06594057, 0.02281303, ..., 0.29399388, 0.06620404,\n       0.42597067])"}, "execution_count": 337, "metadata": {}, "output_type": "execute_result"}, {"data": {"text/plain": "399526"}, "execution_count": 337, "metadata": {}, "output_type": "execute_result"}], "source": "#ESTIMATE HOW MANY CUSTOMERS THE MODEL PREDICTED THAT WILL BUY THE HIGHER BUNDLE\npred_y_propab = lgbm.predict_proba(X_test)\npred_y_propab[:,1]\ncondition = pred_y_propab[:,1] >= 0.5\nlen(pred_y_propab[:,1][condition])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "shap_values = feature_importance(X_test, lgbm)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "shap.summary_plot(shap_values, X_test, feature_names, plot_type=\"bar\")\n    \nshap.summary_plot(shap_values[1], X_test, feature_names, plot_type=\"dot\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### Decile factors"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "from collections import Counter\ny_pred_prob = lgbm.predict_proba(X_test)\ny_pred = lgbm.predict(X_test)\n\ny_test_quart = pd.Series(y_test)\ny_test_quart = pd.DataFrame(y_test)\ny_test_quart['Pred_Target'] = y_pred\ny_test_quart['Prob_1'] = y_pred_prob[:,1]\ny_test_quart = y_test_quart.sort_values(by = \"Prob_1\" , ascending = False)\ny_test_quart.rename(columns = {0:'Actual_Target'}, inplace = True) \n#y_test_quart.head()\n#y_test_quart.tail()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "quartiles = np.array_split(y_test_quart['Prob_1'].values,20)\n#quartiles\nquarts = []\nfor i in list(range(1,21)):\n    temp = [i]*len(quartiles[i-1])\n    quarts.append(temp)   \n    quart = [item for sublist in quarts for item in sublist]\ny_test_quart['Percentile'] = quart\n# y_test_quart.head() "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1 decile:  0.21658854259849\n1,2 decile:  0.3716677997638387\n1,2,3 decile:  0.4878162235660357\n1,2,3,4 decile:  0.5859305113250081\n1,2,3,4,5 decile:  0.6649729845779512\n"}], "source": "print (\"1 decile: \", y_test_quart[y_test_quart.Percentile == 1]['Actual_Target'].value_counts()[1]/Counter(y_test)[1])\nprint (\"1,2 decile: \", y_test_quart[y_test_quart.Percentile.isin([1,2]) & (y_test_quart.Prob_1 > 0.5)]['Actual_Target'].value_counts()[1]/Counter(y_test)[1])\nprint (\"1,2,3 decile: \", y_test_quart[y_test_quart.Percentile.isin([1,2,3]) & (y_test_quart.Prob_1 > 0.5)]['Actual_Target'].value_counts()[1]/Counter(y_test)[1])\nprint (\"1,2,3,4 decile: \", y_test_quart[y_test_quart.Percentile.isin([1,2,3,4]) & (y_test_quart.Prob_1 > 0.5)]['Actual_Target'].value_counts()[1]/Counter(y_test)[1])\nprint (\"1,2,3,4,5 decile: \", y_test_quart[y_test_quart.Percentile.isin([1,2,3,4,5]) & (y_test_quart.Prob_1 > 0.5)]['Actual_Target'].value_counts()[1]/Counter(y_test)[1])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#y_test_quart.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#y_test_quart[y_test_quart['Prob_1'] >= 0.5].shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#y_test_quart[y_test_quart['Prob_1'] >= 0.6].shape"}, {"cell_type": "markdown", "metadata": {}, "source": "# -----------------------------------------------------------------------------------------------------------#"}, {"cell_type": "markdown", "metadata": {}, "source": "### Scoring_dataset <a class=\"anchor\" id=\"Scoring_dataset\"></a>"}, {"cell_type": "markdown", "metadata": {}, "source": "### gdpr3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#month_val[0]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "day_max = 1\ngdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(gdpr, year = year_val[0], month = month_val[0]).select('MSISDN_CLI', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "gdpr = 'gs://'+ permsandprefs_rawprepared_bucket +'/ope_cpm_consent/'+ version +'/parquet/year={}/month={}/day={}/'\n\ndf_gdpr = read_in_data2(gdpr, year = year_val[0], month = month_val[0], day=day_max)"}, {"cell_type": "markdown", "metadata": {}, "source": "### status3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# find max day\nday_max = 1\nstatus = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/'\n\n#print(year_val[2], month_val[2])\nlocals()[\"find_day\"] = read_in_data2(status, year = year_val[0], month = month_val[0]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_status/2.0/parquet/year={}/month={}/day={}/'\n\ndf_status = read_in_data2(status, year = year_val[0], month = month_val[0], day= day_max).select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#df_status = df_status.select(\"MSISDN\", \"TARIFF_PLAN\", \"CONNECTION_DAY\", \"SMARTPHONE_FLAG\", \"INSERTED\", \"STATUS\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_status = df_status.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_gdpr.createOrReplaceTempView(\"gdpr_view\")\ndf_status.createOrReplaceTempView(\"status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# inner join status with gdpr\nconsent_prepay_status = spark.sql(\"\"\"SELECT A.*\n                         FROM (\n                             SELECT substring(MSISDN, 3 , 10) AS MSISDN, TARIFF_PLAN, CONNECTION_DAY, SMARTPHONE_FLAG, INSERTED\n                             FROM status_view A\n                             WHERE STATUS IN ('A','B')\n                             ) AS A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### usage3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# print(month_val[0], year_val[0])\n# print(month_val[1], year_val[1])\n# print(month_val[2], year_val[2])\n# print(month_val[3], year_val[3])\n# print(month_val[4], year_val[4])\n# print(month_val[5], year_val[5])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"text/plain": "[11, 10, 9, 8, 7, 6]"}, "execution_count": 357, "metadata": {}, "output_type": "execute_result"}], "source": "month_val[:-2]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"ename": "AnalysisException", "evalue": "'Path does not exist: gs://vfgr-dh-customerprofilecar-rawprepared/car_pp_master_usage/2.0/parquet/year=2022/month=11;'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o22048.parquet.\n: org.apache.spark.sql.AnalysisException: Path does not exist: gs://vfgr-dh-customerprofilecar-rawprepared/car_pp_master_usage/2.0/parquet/year=2022/month=11;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:576)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:559)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:559)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:667)\n\tat sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n", "\nDuring handling of the above exception, another exception occurred:\n", "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-358-85d01ee7f6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"find_day\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_in_data2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSISDN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'day'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"day_max\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"find_day\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"day\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAX\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mday_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"day_max\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m<ipython-input-14-0650000bd3d0>\u001b[0m in \u001b[0;36mread_in_data2\u001b[0;34m(in_file, year, month, day)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_in_data2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \"\"\"\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/anaconda/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: gs://vfgr-dh-customerprofilecar-rawprepared/car_pp_master_usage/2.0/parquet/year=2022/month=11;'"]}], "source": "# find max day available in dataset for each month\nk = 0\nday_max = []\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[:-2],month_val[:-2]):\n    k=k+1\n    locals()[\"find_day\"+str(k)] = read_in_data2(usage, year = i, month = j).select('MSISDN', 'day')\n    locals()[\"day_max\"+str(k)] = locals()[\"find_day\"+str(k)].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\n    day_max.append(locals()[\"day_max\"+str(k)])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Read datasets from the previous 6-month period\nk = 0 \nday = 1\nusage = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_pp_master_usage/2.0/parquet/year={}/month={}/day={}/'\n\nfor i,j,m in zip(year_val[:-2], month_val[:-2], day_max):\n    k=k+1\n    locals()[\"usage_m\"+str(k)] = read_in_data2(usage, year = i, month = j, day = m)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for name in usage_m1.schema.names: usage_m1 = usage_m1.withColumnRenamed(name, name.replace('M1', 'M1'))\nfor name in usage_m2.schema.names: usage_m2 = usage_m2.withColumnRenamed(name, name.replace('M1', 'M2'))\nfor name in usage_m3.schema.names: usage_m3 = usage_m3.withColumnRenamed(name, name.replace('M1', 'M3'))\nfor name in usage_m4.schema.names: usage_m4 = usage_m4.withColumnRenamed(name, name.replace('M1', 'M4'))\nfor name in usage_m5.schema.names: usage_m5 = usage_m5.withColumnRenamed(name, name.replace('M1', 'M5'))\nfor name in usage_m6.schema.names: usage_m6 = usage_m6.withColumnRenamed(name, name.replace('M1', 'M6'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_m2 = usage_m2.withColumnRenamed(\"MSISDN\", \"M2_MSISDN\")\nusage_m3 = usage_m3.withColumnRenamed(\"MSISDN\", \"M3_MSISDN\")\nusage_m4 = usage_m4.withColumnRenamed(\"MSISDN\", \"M4_MSISDN\")\nusage_m5 = usage_m5.withColumnRenamed(\"MSISDN\", \"M5_MSISDN\")\nusage_m6 = usage_m6.withColumnRenamed(\"MSISDN\", \"M6_MSISDN\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_m1.createOrReplaceTempView(\"usage_view_m1\")\nusage_m2.createOrReplaceTempView(\"usage_view_m2\")\nusage_m3.createOrReplaceTempView(\"usage_view_m3\")\nusage_m4.createOrReplaceTempView(\"usage_view_m4\")\nusage_m5.createOrReplaceTempView(\"usage_view_m5\")\nusage_m6.createOrReplaceTempView(\"usage_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Take active base for the last three months\nusage_prepay = spark.sql(\"\"\"SELECT substring(A.MSISDN, 3 , 10) AS MSISDN,\n                     A.M1_TTL_OUT_CALLS, A.M1_TTL_OUT_MINUTES,\n                     A.M1_TTL_OUT_REVENUE, A.M1_V_CALLS_TO_FIXED_LINES, \n                     A.M1_V_MINUTES_TO_FIXED_LINES, A.M1_V_REVENUE_TO_FIXED_LINES, \n                     A.M1_V_CALLS_TO_COMPETITION, A.M1_V_MINUTES_TO_COMPETITION, A.M1_V_REVENUE_TO_COMPETITION,\n                     A.M1_V_CALLS_TO_INTERNATIONAL, A.M1_V_MINUTES_TO_INTERNATIONAL, \n                     A.M1_V_REVENUE_TO_INTERNATIONAL, A.M1_GPRS_SESSION,\n                     A.M1_GPRS_VOLUME, A.M1_GPRS_REVENUE, A.M1_RECHARGES_NUMBER, \n                     A.M1_RECHARGES_VALUE, A.M1_OUT_DAYS, A.M1_INC_DAYS, A.M1_BUNDLE_REVENUE,\n                     \n                     B.M2_TTL_OUT_CALLS, B.M2_TTL_OUT_MINUTES, \n                     B.M2_TTL_OUT_REVENUE, B.M2_V_CALLS_TO_FIXED_LINES,\n                     B.M2_V_MINUTES_TO_FIXED_LINES, B.M2_V_REVENUE_TO_FIXED_LINES, \n                     B.M2_V_CALLS_TO_COMPETITION, B.M2_V_MINUTES_TO_COMPETITION, \n                     B.M2_V_REVENUE_TO_COMPETITION, B.M2_V_CALLS_TO_INTERNATIONAL,\n                     B.M2_V_MINUTES_TO_INTERNATIONAL, B.M2_V_REVENUE_TO_INTERNATIONAL, \n                     B.M2_GPRS_SESSION, B.M2_GPRS_VOLUME, \n                     B.M2_GPRS_REVENUE, B.M2_RECHARGES_NUMBER, \n                     B.M2_RECHARGES_VALUE, B.M2_OUT_DAYS,\n                     B.M2_INC_DAYS, B.M2_BUNDLE_REVENUE,\n                     \n                     C.M3_TTL_OUT_CALLS, C.M3_TTL_OUT_MINUTES, \n                     C.M3_TTL_OUT_REVENUE, C.M3_V_CALLS_TO_FIXED_LINES,\n                     C.M3_V_MINUTES_TO_FIXED_LINES, C.M3_V_REVENUE_TO_FIXED_LINES, \n                     C.M3_V_CALLS_TO_COMPETITION, C.M3_V_MINUTES_TO_COMPETITION, \n                     C.M3_V_REVENUE_TO_COMPETITION, C.M3_V_CALLS_TO_INTERNATIONAL,\n                     C.M3_V_MINUTES_TO_INTERNATIONAL, C.M3_V_REVENUE_TO_INTERNATIONAL, \n                     C.M3_GPRS_SESSION, C.M3_GPRS_VOLUME, \n                     C.M3_GPRS_REVENUE, C.M3_RECHARGES_NUMBER, \n                     C.M3_RECHARGES_VALUE, C.M3_OUT_DAYS,\n                     C.M3_INC_DAYS, C.M3_BUNDLE_REVENUE,\n                     \n                     D.M4_TTL_OUT_CALLS, D.M4_TTL_OUT_MINUTES, \n                     D.M4_TTL_OUT_REVENUE, D.M4_V_CALLS_TO_FIXED_LINES,\n                     D.M4_V_MINUTES_TO_FIXED_LINES, D.M4_V_REVENUE_TO_FIXED_LINES, \n                     D.M4_V_CALLS_TO_COMPETITION, D.M4_V_MINUTES_TO_COMPETITION, \n                     D.M4_V_REVENUE_TO_COMPETITION, D.M4_V_CALLS_TO_INTERNATIONAL,\n                     D.M4_V_MINUTES_TO_INTERNATIONAL, D.M4_V_REVENUE_TO_INTERNATIONAL, \n                     D.M4_GPRS_SESSION, D.M4_GPRS_VOLUME, \n                     D.M4_GPRS_REVENUE, D.M4_RECHARGES_NUMBER, \n                     D.M4_RECHARGES_VALUE, D.M4_OUT_DAYS, \n                     D.M4_INC_DAYS, D.M4_BUNDLE_REVENUE,\n                     \n                     E.M5_TTL_OUT_CALLS, E.M5_TTL_OUT_MINUTES, \n                     E.M5_TTL_OUT_REVENUE, E.M5_V_CALLS_TO_FIXED_LINES,\n                     E.M5_V_MINUTES_TO_FIXED_LINES, E.M5_V_REVENUE_TO_FIXED_LINES, \n                     E.M5_V_CALLS_TO_COMPETITION, E.M5_V_MINUTES_TO_COMPETITION, \n                     E.M5_V_REVENUE_TO_COMPETITION, E.M5_V_CALLS_TO_INTERNATIONAL,\n                     E.M5_V_MINUTES_TO_INTERNATIONAL, E.M5_V_REVENUE_TO_INTERNATIONAL, \n                     E.M5_GPRS_SESSION, E.M5_GPRS_VOLUME, \n                     E.M5_GPRS_REVENUE, E.M5_RECHARGES_NUMBER, \n                     E.M5_RECHARGES_VALUE, E.M5_OUT_DAYS,\n                     E.M5_INC_DAYS, E.M5_BUNDLE_REVENUE,\n                  \n                     F.M6_TTL_OUT_CALLS, F.M6_TTL_OUT_MINUTES, \n                     F.M6_TTL_OUT_REVENUE, F.M6_V_CALLS_TO_FIXED_LINES,\n                     F.M6_V_MINUTES_TO_FIXED_LINES, F.M6_V_REVENUE_TO_FIXED_LINES, \n                     F.M6_V_CALLS_TO_COMPETITION, F.M6_V_MINUTES_TO_COMPETITION, \n                     F.M6_V_REVENUE_TO_COMPETITION, F.M6_V_CALLS_TO_INTERNATIONAL,\n                     F.M6_V_MINUTES_TO_INTERNATIONAL, F.M6_V_REVENUE_TO_INTERNATIONAL, \n                     F.M6_GPRS_SESSION, F.M6_GPRS_VOLUME, \n                     F.M6_GPRS_REVENUE, F.M6_RECHARGES_NUMBER, \n                     F.M6_RECHARGES_VALUE, F.M6_OUT_DAYS,\n                     F.M6_INC_DAYS, F.M6_BUNDLE_REVENUE\n                     \n                     from usage_view_m1 A\n                     left join usage_view_m2 B\n                          on A.MSISDN = B.M2_MSISDN \n                     left join usage_view_m3 C\n                          on A.MSISDN = C.M3_MSISDN\n                     left join usage_view_m4 D\n                          on A.MSISDN = D.M4_MSISDN\n                     left join usage_view_m5 E\n                          on A.MSISDN = E.M5_MSISDN\n                     left join usage_view_m6 F\n                          on A.MSISDN = F.M6_MSISDN\n                          \n                     WHERE ((A.M1_TTL_OUT_CALLS > 0) or (A.M1_GPRS_SESSION > 0))\"\"\") "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_prepay = usage_prepay.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_prepay.createOrReplaceTempView(\"usage_prepay_view\")\ndf_gdpr.createOrReplaceTempView(\"gdpr_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# join usage with consent\nconsent_prepay_usage = spark.sql(\"\"\"SELECT A.*\n                         FROM usage_prepay_view A\n                         INNER JOIN \n                             (SELECT substring(MSISDN_CLI, 3 , 10) AS MSISDN, MAX(CONSENT_VALID_FROM_DATE) AS CONSENT_VALID_FROM_DATE\n                              from gdpr_view\n                              WHERE asset_status = 'Active' and CURRENT_IND = '1' and advanced_permission != 'NO'\n                              GROUP BY MSISDN_CLI\n                             ) AS B\n                         ON A.MSISDN = B.MSISDN\n                         ORDER BY A.MSISDN \"\"\") "}, {"cell_type": "markdown", "metadata": {}, "source": "# join status with usage"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "consent_prepay_usage.createOrReplaceTempView(\"consent_prepay_usage_view\")\nconsent_prepay_status.createOrReplaceTempView(\"consent_prepay_status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "base_usage_status = spark.sql(\"\"\"SELECT A.*, B.TARIFF_PLAN, B.CONNECTION_DAY, B.SMARTPHONE_FLAG, B.INSERTED\n                             FROM consent_prepay_usage_view A\n                             INNER JOIN consent_prepay_status_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])\n    elif (dict(base_usage_status.dtypes)[c] == 'object' or dict(base_usage_status.dtypes)[c] == 'string' or dict(base_usage_status.dtypes)[c] == 'timestamp'):\n        base_usage_status = base_usage_status.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### Add/Convert_Features3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 1. Convert from second -> minutes\n# 2. Convert from KByte -> MByte\nfor column in base_usage_status.columns:\n    if 'MINUTES' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/60)\n    if 'VOLUME' in column:\n        base_usage_status = base_usage_status.withColumn(column, col(column)/1024)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Group Tariffs"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# CU\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba40', 'CU'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Cuba', 'CU'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# VFPP\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'VALCBASE', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'HAM', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Advanced', 'VFPP'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'OCFP', 'VFPP'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ETHNIC (INTERNATIONAL + TAZA)\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'INTPACK', 'INTERNATIONAL'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'Taza', 'TAZA'))\nbase_usage_status = base_usage_status.withColumn('TARIFF_PLAN', regexp_replace('TARIFF_PLAN', 'TAZA', 'TAZA'))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Tenure"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Calculate tenure in months\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", round(F.months_between(col(\"INSERTED\"), col(\"CONNECTION_DAY\"))))\nbase_usage_status = base_usage_status.withColumn(\"TENURE_IN_MONTHS\", col(\"TENURE_IN_MONTHS\").cast(IntegerType()))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# SELECT ONLY CUSTOMERS THAT ARE MORE THAN 3 MONTHS IN OUR DATABASE\nbase_usage_status = base_usage_status[base_usage_status[\"TENURE_IN_MONTHS\"] > 3]"}, {"cell_type": "markdown", "metadata": {}, "source": "# Average talk per time"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1,7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+ str(month) + \"_MINUTES_PER_CALL\", \n                                                     col(\"M\" + str(month) + \"_TTL_OUT_MINUTES\") / col(\"M\" + str(month) + \"_TTL_OUT_CALLS\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# ARPU"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+\"_ARPU\", \n                        col(\"M\"+str(month)+\"_TTL_OUT_REVENUE\") + col(\"M\"+str(month)+\"_GPRS_REVENUE\") \n                                                     + col(\"M\"+str(month)+\"_BUNDLE_REVENUE\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# ROC"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        prev_month_avg = (col(\"M2\"+column[2:]) + col(\"M3\"+column[2:]) + col(\"M4\"+column[2:]) + col(\"M5\"+column[2:]) + col(\"M6\"+column[2:])) / 5 \n        base_usage_status = base_usage_status.withColumn(\"M1_ROC\"+column[2:], (col(\"M1\"+column[2:]) -  prev_month_avg) / prev_month_avg )"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Average for all and half period"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\nfor column in base_usage_status.schema.names:\n    if ((dict(base_usage_status.dtypes)[column] == 'int64' or dict(base_usage_status.dtypes)[column] == 'double' or dict(base_usage_status.dtypes)[column] == 'int') and column!='MSISDN' and column!= 'TENURE_IN_MONTHS' and column[2:6]!=\"_ROC\"):\n        # first semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M13_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:]))/3)\n        # second semi-semester\n        base_usage_status = base_usage_status.withColumn(\"M46_AVG\"+ column[2:], (col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:]))/3)\n        # calculate the average for all six months     \n        base_usage_status = base_usage_status.withColumn(\"M16_AVG\"+ column[2:], (col(\"M1\"+column[2:])+col(\"M2\"+column[2:])+col(\"M3\"+column[2:])+col(\"M4\"+column[2:])+col(\"M5\"+column[2:])+col(\"M6\"+column[2:])) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Ratio reacharge/ bundle value "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for month in range(1, 7):\n    base_usage_status = base_usage_status.withColumn(\"M\"+str(month)+ \"_EXPENDITURE_RATIO\",\n                                                     col(\"M\"+str(month)+ \"_RECHARGES_VALUE\") / col(\"M\"+str(month)+ \"_BUNDLE_REVENUE\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Fill occured NaNs\nfor c in base_usage_status.columns:\n    if (dict(base_usage_status.dtypes)[c] == 'int64' or dict(base_usage_status.dtypes)[c] == 'double' or\n       dict(base_usage_status.dtypes)[c] == 'int'):\n        base_usage_status = base_usage_status.na.fill(value=0, subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### demographics3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# CAR LINE DATASET\nline = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/'\n\n# find max day\nlocals()[\"find_day\"] = read_in_data2(line, year = year_val[0], month = month_val[0]).select('MSISDN', 'day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "line = 'gs://' + customerprofilecar_rawprepared_bucket + '/car_line/1.0/parquet/year={}/month={}/day={}/'\n\ndf_line = read_in_data2(line, year = year_val[0], month = month_val[0], day= day_max)\ndf_line = df_line.select(\"MSISDN\", \"RETAIL_CUST_ACCT_DWH_ID\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\",\"RETAIL_CUST_ACCT_DWH_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_line = df_line.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "base_usage_status.createOrReplaceTempView(\"base_usage_view\")\ndf_line.createOrReplaceTempView(\"line_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Join CAR_LINE with BASE_USAGE\nusage_status_df = spark.sql (\"\"\"SELECT A.*, B.RETAIL_CUST_ACCT_DWH_ID\n                          FROM base_usage_view A\n                          INNER JOIN line_view B\n                             ON A.MSISDN = B.MSISDN\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# DEMOGRAPHICS DATASET\ndemographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(demographics, year = year_val[0], month = month_val[0]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "demographics = 'gs://'+ customerprofilecar_rawprepared_bucket +'/car_pega_customer/1.0/parquet/year={}/month={}/day={}/'\n\ndf_demographics = read_in_data2(demographics, year = year_val[0], month = month_val[0], day= day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# select specific columns\ndf_demographics = df_demographics.select(\"CUST_DWH_ID\", \"POST_CODE\", \"GENDER\", \"AGE\", \"VF_COMBO_FLG\", \"ACTIVE_TOTAL_LINES\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.dropDuplicates([\"CUST_DWH_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn('POST_CODE', regexp_replace('POST_CODE', 'XXXXX', 'DUMMY'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if (dict(df_demographics.dtypes)[column] == 'int64' or dict(df_demographics.dtypes)[column] == 'double' or\n       dict(df_demographics.dtypes)[column] == 'int'):\n        # fill with mean\n        mean = df_demographics.agg({column: \"avg\"}).collect()[0][0]\n        df_demographics = df_demographics.na.fill(mean, subset=[column])\n    elif (dict(df_demographics.dtypes)[column] == 'object' or dict(df_demographics.dtypes)[column] == 'string'):\n        if (column == \"GENDER\"):\n            df_demographics = df_demographics.na.fill(value=\"O\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n        if (column == \"POST_CODE\"):\n            df_demographics = df_demographics.na.fill(value=\"DUMMY\", subset=[column])\n            df_demographics = df_demographics.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n        df_demographics = df_demographics.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# STRANGE VALUES FOR AGES\ndf_demographics = df_demographics.withColumn(\"AGE\", coalesce(col(\"AGE\"), lit(0.0)))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") > 80, 80).otherwise(col(\"AGE\")))\ndf_demographics = df_demographics.withColumn(\"AGE\", when(col(\"AGE\") < 17, 18).otherwise(col(\"AGE\")))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join usage-status with demographics"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_demographics.createOrReplaceTempView(\"demographics_view\")\nusage_status_df.createOrReplaceTempView(\"usage_status_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# JOIN\nusage_status_demo = spark.sql (\"\"\"SELECT A.*, B.POST_CODE, B.GENDER, B.AGE, B.VF_COMBO_FLG, B.ACTIVE_TOTAL_LINES\n                          FROM usage_status_view A\n                          LEFT JOIN demographics_view B\n                             ON A.RETAIL_CUST_ACCT_DWH_ID = B.CUST_DWH_ID\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo = usage_status_demo.drop(col(\"RETAIL_CUST_ACCT_DWH_ID\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in df_demographics.columns:\n    if column != 'CUST_DWH_ID':\n        if (dict(usage_status_demo.dtypes)[column] == 'int64' or dict(usage_status_demo.dtypes)[column] == 'double' or\n       dict(usage_status_demo.dtypes)[column] == 'int'):\n            # fill with mean\n            mean = usage_status_demo.agg({column: \"avg\"}).collect()[0][0]\n            usage_status_demo = usage_status_demo.na.fill(mean, subset=[column])\n        elif (dict(usage_status_demo.dtypes)[column] == 'object' or dict(usage_status_demo.dtypes)[column] == 'string'):\n            if (column == \"GENDER\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"O\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"O\").otherwise(col(column)))\n            if (column == \"POST_CODE\"):\n                usage_status_demo = usage_status_demo.na.fill(value=\"DUMMY\", subset=[column])\n                usage_status_demo = usage_status_demo.withColumn(column, when(col(column)== \"\" ,\"DUMMY\").otherwise(col(column)))\n            usage_status_demo = usage_status_demo.na.fill(value=\"N/A\", subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### post_code3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "subprocess.call('/bin/sh /usr/bin/gsutil -q cp gs://' + files_bucket + '/notebooks/jupyter/higher_bundles/Sociodemographics.xlsx Sociodemographics.xlsx', shell=True)\nPopulation_pools = pd.read_excel('Sociodemographics.xlsx')\npopulation_pools_df = sql.createDataFrame(Population_pools)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.drop(\"Postcode_key\", \"Name\", \"Periferiaki_enotita\", \"Population_aged_60+\",\n                                              \"Male_Population_aged_60+\", \"Female_Population_aged_60+\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df = population_pools_df.withColumnRenamed('Population_aged_0-14', 'Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_0-14', 'Male_Population_aged_0_14')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_0-14', 'Female_Population_aged_0_14')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_15-29', 'Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_15-29', 'Male_Population_aged_15_29')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_15-29', 'Female_Population_aged_15_29')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_30-44', 'Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_30-44', 'Male_Population_aged_30_44')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_30-44', 'Female_Population_aged_30_44')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Population_aged_45-59', 'Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Male_Population_aged_45-59', 'Male_Population_aged_45_59')\npopulation_pools_df = population_pools_df.withColumnRenamed('Female_Population_aged_45-59', 'Female_Population_aged_45_59')\n\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_million_Euro', 'Purchasing_Power_million_Euro')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_per_mill_of_country', 'Purchasing_Power_per_mill_of_country')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_Euro_per_capita', 'Purchasing_Power_Euro_per_capita')\npopulation_pools_df = population_pools_df.withColumnRenamed('Purchasing_Power:_index_(country_eq.100)', 'Purchasing_Power_index_country_eq_100')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "population_pools_df.createOrReplaceTempView(\"population_pools_view\")\nusage_status_demo.createOrReplaceTempView(\"usage_status_demo_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Exclude ages from 60+\nusage_status_demo_pc = spark.sql (\"\"\"SELECT A.*, B.Population, B.Households, B.Average_Household_Size, B.Male_Population,\n                          B.Female_Population, B.Population_aged_0_14, B.Male_Population_aged_0_14, B.Female_Population_aged_0_14,\n                          B.Population_aged_15_29, B.Male_Population_aged_15_29, B.Female_Population_aged_15_29, \n                          B.Population_aged_30_44, B.Male_Population_aged_30_44, B.Female_Population_aged_30_44,\n                          B.Population_aged_45_59, B.Male_Population_aged_45_59, B.Female_Population_aged_45_59,\n                          B.Purchasing_Power_million_Euro, B.Purchasing_Power_per_mill_of_country,\n                          B.Purchasing_Power_Euro_per_capita, B.Purchasing_Power_index_country_eq_100\n\n                          FROM usage_status_demo_view A                          \n                          LEFT JOIN population_pools_view B                          \n                          ON A.POST_CODE = B.POST_CODE\n                       \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Fill NaNs\nfor column in population_pools_df.columns:\n    usage_status_demo_pc = usage_status_demo_pc.na.fill(value=0, subset=[column])"}, {"cell_type": "markdown", "metadata": {}, "source": "### students3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(status_service, year = year_val[0], month = month_val[0]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/day={}'\n\ndf_status_service = read_in_data2(status_service, year = year_val[0], month = month_val[0], day=day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_status_service.createOrReplaceTempView(\"df_status_service_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_students = spark.sql(\"\"\"SELECT * \n                           FROM df_status_service_view A\n                           WHERE SERVICE_CODE == 'BDLCUPaso' \n                           \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_students = df_students.dropDuplicates([\"MSISDN\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ADD A NEW COLUMN WITH A FLAG TO INIDICATE THAT THIS USER IS STUDENT\ndf_students = df_students.withColumn(\"STUDENTS_FLAG\", lit(\"Y\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "# Left join student info with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc.createOrReplaceTempView(\"usage_status_demo_pc_view\")\ndf_students.createOrReplaceTempView(\"students_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud = spark.sql(\"\"\"SELECT A.*, B.STUDENTS_FLAG\n                                FROM usage_status_demo_pc_view A\n                                LEFT JOIN students_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# SET indicator N for NO if the user is not a students\nusage_status_demo_pc_stud = usage_status_demo_pc_stud.na.fill(value=\"N\", subset=[\"STUDENTS_FLAG\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### buckets3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#month_val[:2]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 2-month period\nk = 0 \nday = 1\nbuckets = 'gs://'+ model_outputs_bucket + '/prepay_buckets/result/parquet/1.0/year={}/month={}/'\n\nfor i,j in zip(year_val[:2], month_val[:2]):\n    k=k+1\n    locals()[\"buckets_m\"+str(k)] = read_in_data2(buckets, year = i, month = j)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets = buckets_m1.union(buckets_m2)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets = df_buckets.sort(col(\"BUNDLE\").asc(), col(\"ACTIVATION_DATE\").desc())"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep first occurrence for each bundle\ndf_buckets = df_buckets.select(\"MSISDN\", \"ACTIVATION_DATE\", \"BUNDLE\", \"BUNDLE_REVENUE\", \"BALANCE\", \"VOICE_BUCKET\", \"DATA_BUCKET\", \"SMS_BUCKET\",\n                                   F.row_number().over(Window.partitionBy(\"MSISDN\", \"BUNDLE\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_buckets = df_buckets.filter(col(\"row_num\") == 1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Calculate summary for all buckets\ndf_buckets_summary = df_buckets.groupBy(\"MSISDN\").agg(sum(\"VOICE_BUCKET\").alias(\"VOICE_BUCKET_SUMMARY\"),sum(\"DATA_BUCKET\").alias(\"DATA_BUCKET_SUMMARY\"),sum(\"SMS_BUCKET\").alias(\"SMS_BUCKET_SUMMARY\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "### balance3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "### drop duplicates and keep last registration \ndf_balance = df_buckets.select(\"MSISDN\", \"BALANCE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"ACTIVATION_DATE\"))).alias(\"row_num\"))\n\ndf_balance_summary = df_balance.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join buckets with balance"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets_summary.createOrReplaceTempView(\"buckets_sum_view\")\ndf_balance_summary.createOrReplaceTempView(\"balance_sum_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_buckets_balance = spark.sql(\"\"\"SELECT A.*, B.BALANCE\n                             FROM buckets_sum_view A\n                             INNER JOIN balance_sum_view B\n                                  ON A.MSISDN = B.MSISDN \"\"\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join bucket-balance with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud.createOrReplaceTempView(\"usage_status_demo_pc_stud_view\")\ndf_buckets_balance.createOrReplaceTempView(\"buckets_balance_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets = spark.sql(\"\"\"SELECT A.*, B.VOICE_BUCKET_SUMMARY, B.DATA_BUCKET_SUMMARY, B.SMS_BUCKET_SUMMARY, B.BALANCE\n                                FROM usage_status_demo_pc_stud_view A\n                                LEFT JOIN buckets_balance_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for c in df_buckets_balance.columns:\n    if (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'int64' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'double' or\n       dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'int'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=0, subset=[c])\n    elif (dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'object' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'string' or dict(usage_status_demo_pc_stud_buckets.dtypes)[c] == 'timestamp'):\n        usage_status_demo_pc_stud_buckets = usage_status_demo_pc_stud_buckets.na.fill(value=\"N/A\", subset=[c])"}, {"cell_type": "markdown", "metadata": {}, "source": "### drop_calls3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "drop_calls = 'gs://'+ mediatedcdrs_bucket +'/eds_network_cdr/2.0/parquet/year={}/month={}/'\n\ndf_drop_calls = read_in_data2(drop_calls, year = year_val[0], month = month_val[0])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_drop_calls = df_drop_calls.select(\"SAMPLED\",\"A_NUMBER\",\"FIRST_LAC\",\"LAST_LAC\",\"CELL\",\"LAST_CELL\",\"REC_TYPE\",\"TARIFF\",\n                               \"DURATION\", \"TERM_CAUSE\", \"day\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_drop_calls.createOrReplaceTempView(\"cdrs_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "drop_calls_query = spark.sql(\"\"\"SELECT K.MSISDN,\n                                count(*) as DROPPED_CALL_COUNT\n                                FROM (\n                                    SELECT L.MSISDN, L.SAMPLED, L.YEAR\n                                    FROM (\n                                       SELECT\n                                       A.SAMPLED, A.A_NUMBER AS MSISDN, A.day, YEAR(A.SAMPLED) AS YEAR, SUBSTR(A.TERM_CAUSE,1,4) AS EOS,\n                                       CASE WHEN A.FIRST_LAC LIKE '%IE%' THEN A.LAST_LAC  --this condition holds only for calls\n                                           WHEN A.FIRST_LAC = '' THEN A.LAST_LAC          --this condition holds only for calls\n                                           ELSE A.FIRST_LAC END FIRST_LAC,                --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_LAC = '' THEN A.FIRST_LAC         --this condition holds for both calls and SMS\n                                           ELSE A.LAST_LAC END LAST_LAC,                  --this condition holds for both calls and SMS\n                                       CASE WHEN A.CELL LIKE '%F%' THEN A.LAST_CELL       --this condition holds only for calls\n                                           WHEN A.CELL = '' THEN A.LAST_CELL              --this condition holds only for calls\n                                           ELSE A.CELL END CELL,                          --this condition holds for both calls and SMS\n                                       CASE WHEN A.LAST_CELL = '' THEN A.CELL             --this condition holds for both calls and SMS\n                                           ELSE A.LAST_CELL END LAST_CELL,                --this condition holds for both calls and SMS\n                                       ROW_NUMBER() OVER (PARTITION BY A.A_NUMBER, A.SAMPLED ORDER BY A.A_NUMBER, A.SAMPLED) as LEVEL\n                                       FROM cdrs_view A\n                                       WHERE ((A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL != '') OR  -- this condition holds only for calls\n                                       (A.FIRST_LAC = '' AND A.LAST_LAC != '' AND A.CELL = '' AND A.LAST_CELL != '') OR    -- this condition holds only for calls\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC = '' AND A.CELL != '' AND A.LAST_CELL = '') OR    -- this condition holds for both calls and SMS\n                                       (A.FIRST_LAC != '' AND A.LAST_LAC != '' AND A.CELL != '' AND A.LAST_CELL = ''))     -- this condition holds for both calls and SMS\n                                       AND A.REC_TYPE IN ('20','30') AND A.TARIFF != '142'\n                                       AND LENGTH(A.TERM_CAUSE) = 8\n                                       AND SUBSTR(A.TERM_CAUSE,1,4) IN ('068F','08BF','09A6','09C3','09C5','09C8','09F8','0A0E','0A0F','0AE9','0C15','0CD2',\n                                                                        '0CD3','0F7B','0F7C','018F','065D','065E','0700','0701','0702','09A7','09BF','09C0',\n                                                                        '09C2','09C4','09C6','09C7','09C9','09F6','09F7','0A0A','0A0B','0A0C','0A0D','0C14',\n                                                                        '0C16','0F7D','1C8F','1C90','1C91','1C92','1C9A','1C9B')\n                                       AND A.A_NUMBER != '' AND A.A_NUMBER LIKE '69%' \n                                       --ORDER BY A.A_NUMBER, A.SAMPLED\n                                       ) AS L\n                                    WHERE L.LEVEL = 1\n                                    --ORDER BY L.MSISDN, L.SAMPLED ASC\n                                ) K\n                                GROUP BY K.MSISDN\n                                ORDER BY K.MSISDN\n                             \"\"\")  "}, {"cell_type": "markdown", "metadata": {}, "source": "# Join drop_calls with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_view\")\ndrop_calls_query.createOrReplaceTempView(\"drop_calls_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = spark.sql(\"\"\"SELECT A.*, B.DROPPED_CALL_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_view A\n                                LEFT JOIN drop_calls_view B\n                                ON A.MSISDN=B.MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls = usage_status_demo_pc_stud_buckets_dropcalls.na.fill(value=0, subset=[\"DROPPED_CALL_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### tickets3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ticket_service = 'gs://'+ dhdwh_bucket +'/mobile_sr_tt/1.0/parquet/year={}/month={}/'\n\ndf_tickets_requests = read_in_data2(ticket_service, year = year_val[0], month = month_val[0]).select(\"X_MSISDN\",\"SR_ID\").drop('service_file_id')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# drop duplicates\ndf_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\", \"SR_ID\"])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# ADD A COLUMN AS TICKETS COUNTER FOR EACH MSISDN\ndf_tickets_requests = df_tickets_requests.select(\"X_MSISDN\", F.count(\"X_MSISDN\").over(Window.partitionBy(\"X_MSISDN\")).alias(\"TICKETS_COUNT\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_tickets_requests = df_tickets_requests.dropDuplicates([\"X_MSISDN\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join tickets with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_view\")\ndf_tickets_requests.createOrReplaceTempView(\"tickets_requests_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = spark.sql(\"\"\"SELECT A.*, B.TICKETS_COUNT\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_view A\n                                LEFT JOIN tickets_requests_view B\n                                ON A.MSISDN=B.X_MSISDN\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets = usage_status_demo_pc_stud_buckets_dropcalls_tickets.na.fill(value=0, subset=[\"TICKETS_COUNT\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### channel3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "ticket_service = 'gs://'+ dhdwh_bucket +'/mobile_sr_tt/1.0/parquet/year={}/month={}/'\n\ndf_tickets_requests = read_in_data2(ticket_service, year = year_val[0], month = month_val[0]).drop('service_file_id')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_events.createOrReplaceTempView(\"df_events_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = spark.sql(\"\"\"SELECT ACCOUNT_ID, REQUESTING_SYSTEM                             \n                             FROM df_events_view \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'VOP', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUapp', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'MCare', 'DIGITAL'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CUsite', 'DIGITAL'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Change column names\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'TAZAAPP', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'EKIOSK', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PostpaidToPrepaid', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'XPCVM', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'LMG', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'PEGA', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'SMSVAS', 'OTHER'))\ndf_bundle_purchase = df_bundle_purchase.withColumn('REQUESTING_SYSTEM', regexp_replace('REQUESTING_SYSTEM', 'CRM', 'OTHER'))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# MIA KATHGORIA VFSHOP - (VFSHOP)\n# ALLH KATHGORIA - THLEFWNO (IVR)  \n# DIGITAL (VOP, MCARE, CUapp, CUsite)\n# OTHER"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# One-hot encoding - 4 kathgories\ndf_bundle_purchase = df_bundle_purchase.groupBy('ACCOUNT_ID').pivot('REQUESTING_SYSTEM').count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for c in df_bundle_purchase.columns:\n        df_bundle_purchase = df_bundle_purchase.na.fill(0, subset=[c])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_bundle_purchase = df_bundle_purchase.select(\"ACCOUNT_ID\", \"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\")"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join channel with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_view\")\ndf_bundle_purchase.createOrReplaceTempView(\"bundle_purchase_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = spark.sql(\"\"\"SELECT A.*, B.DIGITAL, B.VFShop, B.IVR, B.OTHER\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_view A\n                                LEFT JOIN bundle_purchase_view B\n                                ON A.MSISDN= SUBSTR(B.ACCOUNT_ID,3,10)\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.na.fill(value=0, subset=[\"DIGITAL\", \"VFShop\", \"IVR\", \"OTHER\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "### NumberOfBundles3"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Read datasets from the previous 6-month period\nk = 0 \nday = 1\nevents = 'gs://'+ mediatedcdrs_bucket + '/alu_prepay_cdr/2.0/parquet/year={}/month={}/'\n\nfor i,j in zip(year_val[:-2], month_val[:-2]):\n    k=k+1\n    locals()[\"events_m\"+str(k)] = read_in_data2(events, year = i, month = j).select(\"ACCOUNT_ID\", \"PTP_COSP_AMA_CODE\", \"EVENT_LABEL\", \"EVENT_RESULT\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "events_m1.createOrReplaceTempView(\"events_view_m1\")\nevents_m2.createOrReplaceTempView(\"events_view_m2\")\nevents_m3.createOrReplaceTempView(\"events_view_m3\")\nevents_m4.createOrReplaceTempView(\"events_view_m4\")\nevents_m5.createOrReplaceTempView(\"events_view_m5\")\nevents_m6.createOrReplaceTempView(\"events_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m1 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m2 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m2 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m3 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m3 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m4 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m4 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m5 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m5 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m6 = spark.sql(\"\"\"SELECT ACCOUNT_ID, COUNT(PTP_COSP_AMA_CODE) AS BUNDLES_NUM                       \n                             FROM events_view_m6 \n                             WHERE ((EVENT_LABEL=139) AND (EVENT_RESULT=169)) \n                             GROUP BY ACCOUNT_ID\n                             \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "df_num_of_bundles_m1.createOrReplaceTempView(\"num_of_bundles_view_m1\")\ndf_num_of_bundles_m2.createOrReplaceTempView(\"num_of_bundles_view_m2\")\ndf_num_of_bundles_m3.createOrReplaceTempView(\"num_of_bundles_view_m3\")\ndf_num_of_bundles_m4.createOrReplaceTempView(\"num_of_bundles_view_m4\")\ndf_num_of_bundles_m5.createOrReplaceTempView(\"num_of_bundles_view_m5\")\ndf_num_of_bundles_m6.createOrReplaceTempView(\"num_of_bundles_view_m6\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = spark.sql(\"\"\"SELECT substring(A.ACCOUNT_ID, 3 , 10) AS ACCOUNT_ID, A.BUNDLES_NUM AS M1_BUNDLES_NUM, \n                     B.BUNDLES_NUM AS M2_BUNDLES_NUM, C.BUNDLES_NUM AS M3_BUNDLES_NUM, \n                     D.BUNDLES_NUM AS M4_BUNDLES_NUM,\n                     E.BUNDLES_NUM AS M5_BUNDLES_NUM, \n                     F.BUNDLES_NUM AS M6_BUNDLES_NUM\n                     \n                     FROM num_of_bundles_view_m1 A                 \n                    \n                     left join num_of_bundles_view_m2 B\n                          on A.ACCOUNT_ID = B.ACCOUNT_ID \n                     left join num_of_bundles_view_m3 C \n                          on A.ACCOUNT_ID = C.ACCOUNT_ID\n                     left join num_of_bundles_view_m4 D \n                          on A.ACCOUNT_ID = D.ACCOUNT_ID\n                     left join num_of_bundles_view_m5 E \n                          on A.ACCOUNT_ID = E.ACCOUNT_ID\n                     left join num_of_bundles_view_m6 F\n                          on A.ACCOUNT_ID = F.ACCOUNT_ID\n                          \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "num_of_bundles_semester = num_of_bundles_semester.na.fill(value=0)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Create 3 month and 6 month averages for all usage columns\n# first semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M13_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\"))/3)\n# second semi-semester\nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M46_AVG_BUNDLES_NUM\", (col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\"))/3)\n# calculate the average for all six months     \nnum_of_bundles_semester = num_of_bundles_semester.withColumn(\"M16_AVG_BUNDLES_NUM\", (col(\"M1_BUNDLES_NUM\")+col(\"M2_BUNDLES_NUM\")+col(\"M3_BUNDLES_NUM\")+col(\"M4_BUNDLES_NUM\")+col(\"M5_BUNDLES_NUM\")+col(\"M6_BUNDLES_NUM\")) /6)"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join number of bundles with main dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel.createOrReplaceTempView(\"usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view\")\nnum_of_bundles_semester.createOrReplaceTempView(\"num_of_bundles_semester_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = spark.sql(\"\"\"SELECT A.*, B.M1_BUNDLES_NUM, B.M2_BUNDLES_NUM, B.M3_BUNDLES_NUM, \n                                B.M4_BUNDLES_NUM, B.M5_BUNDLES_NUM, B.M6_BUNDLES_NUM, B.M13_AVG_BUNDLES_NUM, B.M46_AVG_BUNDLES_NUM, \n                                B.M16_AVG_BUNDLES_NUM\n                                FROM usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_view A\n                                LEFT JOIN num_of_bundles_semester_view B\n                                ON A.MSISDN= B.ACCOUNT_ID\"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.na.fill(value=0)"}, {"cell_type": "markdown", "metadata": {}, "source": "# -----------------------------------------------------------------------------------------------------------#"}, {"cell_type": "markdown", "metadata": {}, "source": "# Pandas scoring dataset"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "scoring_boruta_features = boruta_features\nscoring_boruta_features.remove(\"HIGHER_BUNDLE\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#len(scoring_boruta_features)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scoring_boruta_features"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "final_scoring_df = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.select(scoring_boruta_features).toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#final_scoring_df.columns"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "objects = final_scoring_df.select_dtypes(include=['object'])\nobject_names = list(objects.columns.values)\n\n#print(object_names)\nobject_names.remove('MSISDN')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "le = LabelEncoder()\n\n# Encoding all the categorical columns\nfor object_name in object_names:\n    if (final_scoring_df[object_name].nunique() > 2):\n        enc_pc = pd.get_dummies(final_scoring_df[object_name], drop_first = True)\n        final_scoring_df = final_scoring_df.drop(object_name, axis = 1)\n        final_scoring_df = pd.concat([final_scoring_df, enc_pc], axis = 1)\n    elif (final_scoring_df[object_name].nunique() <= 2):\n        le.fit(final_scoring_df[object_name].astype(str))\n        final_scoring_df[object_name] = le.transform(final_scoring_df[object_name].astype(str))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Score <a class=\"anchor\" id=\"score\"></a>"}, {"cell_type": "markdown", "metadata": {}, "source": "# PREPARE FINAL OUTPUT DATASET"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 1. PROPENSITY\nfinal_scoring_df_new = final_scoring_df.drop(\"MSISDN\", axis=1)\nhigher_bundle_propensity = lgbm.predict_proba(final_scoring_df_new)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#higher_bundle_propensity"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# 2. Bring more info"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "msisdn = final_scoring_df[\"MSISDN\"]\nvoice_bucket = final_scoring_df[\"VOICE_BUCKET_SUMMARY\"]\ndata_bucket = final_scoring_df[\"DATA_BUCKET_SUMMARY\"]\nsms_bucket = final_scoring_df[\"SMS_BUCKET_SUMMARY\"]\nstudents_flg = final_scoring_df[\"STUDENTS_FLAG\"] "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_list = list(zip(msisdn,voice_bucket,data_bucket,sms_bucket,students_flg, list(higher_bundle_propensity[:,1])))\nsortED = list(sorted(output_list, key=lambda x: x[5], reverse=True))\nscored = pd.DataFrame(sortED, columns = ['MSISDN','VOICE_BUCKET_SUMMARY', 'DATA_BUCKET_SUMMARY', 'SMS_BUCKET_SUMMARY', 'STUDENTS_FLAG','PROPENSITY'])"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scored['PROPENSITY'] =  round(scored['PROPENSITY'], 4)\nscored['PROPENSITY'] =  scored['PROPENSITY'].round(3)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# quartiles = np.array_split(scored['PROPENSITY'].values,10)\n# quarts = []\n# for i in list(range(1,11)):\n#     temp = [i]*len(quartiles[i-1])\n#     quarts.append(temp)\n# quart = [item for sublist in quarts for item in sublist]\n# scored['PERCENTILE'] = quart\n\n# scored[scored['PROPENSITY'] >= 0.5].shape\n# scored['PROPENSITY'] =  round(scored['PROPENSITY'], 4)\n# scored.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Add TARIFF PLAN information"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scored.shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "help_df = usage_status_demo_pc_stud_buckets_dropcalls_tickets_channel_nob.select(scoring_boruta_features).toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#help_df.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df = pd.merge(scored, help_df[[\"MSISDN\",\"TARIFF_PLAN\"]], on='MSISDN')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#output_df.shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df[\"TARIFF_PLAN\"] = np.where((output_df[\"TARIFF_PLAN\"] == \"CU\") & (output_df[\"STUDENTS_FLAG\"] == 0), \"CURest\", output_df[\"TARIFF_PLAN\"])\noutput_df[\"TARIFF_PLAN\"] = np.where((output_df[\"TARIFF_PLAN\"] == \"CU\") & (output_df[\"STUDENTS_FLAG\"] == 1), \"CUStudent\", output_df[\"TARIFF_PLAN\"])"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join with the last purchased bundle"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/'\n\nday_max=1\n# find max day\nlocals()[\"find_day\"] = read_in_data2(status_service, year = year_val[0], month = month_val[0]).select('day')\nlocals()[\"day_max\"] = locals()[\"find_day\"].select(F.max(F.col(\"day\")).alias(\"MAX\")).limit(1).collect()[0].MAX\nday_max = (locals()[\"day_max\"])\n#print(day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "status_service = 'gs://'+ dhdwh_bucket +'/master_status_services/1.0/parquet/year={}/month={}/day={}'\n\nrecent_bundles = read_in_data2(status_service, year = year_val[0], month = month_val[0], day=day_max)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "recent_bundles.createOrReplaceTempView(\"recent_bundles_view\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "recent_bundles = spark.sql(\"\"\"SELECT MSISDN, SERVICE_CODE FROM recent_bundles_view\n                WHERE (SERVICE_CODE=\"BDLEthnicDataH\") OR (SERVICE_CODE=\"BDLDataTazaINT\") \n                \n                OR (SERVICE_CODE=\"BDLIntegLPak\") OR (SERVICE_CODE=\"BDLIntegPak\") \n                OR (SERVICE_CODE=\"BDLIntegLInd\") OR (SERVICE_CODE=\"BDLIntegInd\")\n                OR (SERVICE_CODE=\"BDLIntegLBang\") OR (SERVICE_CODE=\"BDLIntegBang\")\n                OR (SERVICE_CODE=\"BDLAlbania\") OR (SERVICE_CODE=\"BDLVFAlbInt\")\n                \n                OR (SERVICE_CODE=\"BDLXNetData\") OR (SERVICE_CODE=\"BDLPreCombo\")\n                \n                OR (SERVICE_CODE=\"BDLTalkText600\") OR (SERVICE_CODE=\"BDLComboMax\") OR (SERVICE_CODE=\"BDLCUComboXL\")                \n                OR (SERVICE_CODE=\"BDLPasoComboH\") OR (SERVICE_CODE=\"BDLPasoComboXL\") OR (SERVICE_CODE=\"BDLPasoComboML\")\n                OR (SERVICE_CODE=\"BDLPasoComboTL\")\n                \"\"\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "recent_bundles = recent_bundles.withColumn(\"PRICE\", when(col(\"SERVICE_CODE\") == \"BDLTalkText600\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLComboMax\", 13.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLCUComboXL\", 15)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboH\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboXL\", 10)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboML\", 12)\n                .when(col(\"SERVICE_CODE\") == \"BDLPasoComboTL\", 17.5)\n                # TAZA\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLPak\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLInd\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegLBang\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLAlbania\", 5.3)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegPak\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegInd\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLIntegBang\", 8.5)\n                .when(col(\"SERVICE_CODE\") == \"BDLVFAlbInt\", 8.5)\n                # INTERNATIONAL\n                .when(col(\"SERVICE_CODE\") == \"BDLEthnicDataH\", 8.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLDataTazaINT\", 10.5)\n                # VFPP\n                .when(col(\"SERVICE_CODE\") == \"BDLXNetData\", 10.9)\n                .when(col(\"SERVICE_CODE\") == \"BDLPreCombo\", 13.5)\n                )"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "recent_bundles = recent_bundles.select(\"MSISDN\", \"SERVICE_CODE\", \"PRICE\", F.row_number().over(Window.partitionBy(\"MSISDN\").orderBy(desc(\"PRICE\"))).alias(\"row_num\"))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "last_bundle = recent_bundles.filter(col(\"row_num\") == 1).drop(\"row_num\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "last_bundle_pd =  last_bundle.toPandas()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Join last bundle with output"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df = output_df.merge(last_bundle_pd, how='left', on='MSISDN')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "for column in output_df.columns.values:\n    if (output_df[column].dtype == 'int64' or output_df[column].dtype == 'float64'):\n        # chech strange age values\n        output_df[column] = output_df[column].fillna(0)\n    elif (output_df[column].dtype == 'object' or output_df[column].dtype == 'str'):\n        output_df[column] = output_df[column].fillna(\"NULL\")"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df['PROPENSITY'] =  output_df['PROPENSITY'].round(3)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df['VOICE_BUCKET_SUMMARY'] =  output_df['VOICE_BUCKET_SUMMARY'].round(3)\noutput_df['DATA_BUCKET_SUMMARY'] =  output_df['DATA_BUCKET_SUMMARY'].round(3)\noutput_df['SMS_BUCKET_SUMMARY'] =  output_df['SMS_BUCKET_SUMMARY'].round(3)\noutput_df['PRICE'] =  output_df['PRICE'].round(3)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#output_df[\"SERVICE_CODE\"].value_counts()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#output_df[output_df[\"SERVICE_CODE\"]== \"NULL\"].shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#output_df.head()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df.isnull().values.any()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df = output_df.replace([np.inf, -np.inf, np.nan], 0.0)\noutput_df = output_df.fillna(0)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df.isnull().values.any()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#output_df.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "# VOLUME SCORING"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# Exclude users with bundle 15 euro\n# output_df = output_df[~((output_df[\"PROPENSITY\"] > 0.5) & (output_df[\"SERVICE_CODE\"] == 'BDLCUComboXL'))]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# People with high propensity to buy higher bundle\n#scoring_volume = output_df[output_df[\"PROPENSITY\"] >= 0.5]"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scoring_volume.shape"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scoring_volume[\"TARIFF_PLAN\"].value_counts()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#scoring_volume[\"SERVICE_CODE\"].value_counts()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#test = scoring_volume.groupby([\"TARIFF_PLAN\", \"SERVICE_CODE\", \"PRICE\"])[\"MSISDN\"].count()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#test.head(100)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "metadata": {}, "source": "### Save model <a class=\"anchor\" id=\"save_model\"></a>"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "file_out = 'gs://'+ output_bucket_new +'/higher_bundles/result/parquet/'+ version +'/year={}/month={}/day={}'"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import datetime\ncurrentDay = datetime.date.today().day\ncurrentMonth = datetime.date.today().month\ncurrentYear = datetime.date.today().year"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#print(currentYear, currentMonth, currentDay)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df_pyspark = spark.createDataFrame(output_df)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df_pyspark.printSchema()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df_pyspark.limit(10).toPandas()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "output_df_pyspark.write.parquet(file_out.format(currentYear, currentMonth, currentDay), mode='overwrite')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "import time\nstop_time = time.time()\nprint(stop_time)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "print(stop_time/60 - start_time/60) "}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}